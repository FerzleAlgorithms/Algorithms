<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Transform-and-Conquer</title>
  <script src="/Algorithms/scripts/chapterScripts.js"></script>
  <link rel="stylesheet" href="/Algorithms/css/style.css">
  <link rel="stylesheet" href="/Algorithms/css/chapter.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DQ5LVZVFDC"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-DQ5LVZVFDC');
</script>
</head>
<body>
  <h1>Transform-and-Conquer</h1>

  <!-- Motivation & Introduction -->
  <section id="introduction">
    <h2>Introduction</h2>
        <p>
      Transform-and-Conquer is a versatile algorithmic strategy that solves a problem by transforming it into a different instance or representation in which the solution is easier or more efficient to compute, then mapping the result back to the original problem. There are three kinds of transformations that are commonly used.
    </p>
    <ul>
       <li><strong>Instance simplification:</strong> Reducing problem size or complexity, 
          often by performing a pre-computation step. A common example is <em>presorting</em>, 
          where sorting the input in \(O(n\log n)\) enables faster operations like binary search 
          in \(O(\log n)\) or uniqueness checks by scanning adjacent items in \(O(n)\).</li>  
          <li><strong>Representation change:</strong> Altering data structures or encodings.
          Two great examples include
          <ul>
          <li>Horner's Rule for polynomial evaluation that rewrites a 
       degree-\(n\) polynomial into a nested form to avoid repeated exponentiations,
        reducing naive \(O(n^2)\) evaluation to \(O(n)\). </li>
        <li>Heapsort transforms an array 
        into a heap in (\(O(n)\)) time and then repeatedly extracts all of the elements in
         \(n\times O(\log n)\) time, achieving \(O(n\log n)\) time overall.</li>
         </ul>
         </li>
           <li><strong>Problem reduction:</strong> 
           Transforming new challenges into well-understood problems to leverage existing solutions. 
           Examples include:
        <ul>
          <li>Mapping bipartite matching to a max-flow problem on a constructed network.</li>
          <li>Reducing all-pairs shortest paths to repeated single-source shortest-path 
          computations (e.g. Dijkstra's or Bellman-Ford).</li>
        </ul>
        These reduction techniques enable solving diverse problems by reusing known algorithms. 
        They also underpin much of computational complexity analysis and will be 
        explored in depth in a dedicated chapter on reductions and complexity theory.
        </li>
    </ul>
  </section>

 <section id="examples">
    <h2>Examples</h2>

 <div class="example-box">
      <strong class="example-title">Example 1: Presorting</strong>
      <p>
        Presorting transforms an unsorted input into a sorted one so that subsequent queries or 
        computations become more efficient. For instance, suppose we need to perform membership 
        queries (that is, searching) on an array of \(n\) values. 
        The straightforward approach (i.e. linear search) tests each value 
        against all keys in \(O(n)\) time per query, yielding \(O(n m)\) time
        for \(m\) queries. Instead, by presorting the keys in \(O(n\log n)\) and then 
        using binary search for each query in \(O(\log n)\), the total cost becomes 
        \(O(n\log n + m\log n)\), which is asymptotically better when \(m\) is large.
      </p>
      <p>
        Another application is the <strong>element uniqueness</strong> problem: 
        determining whether an array of \(n\) numbers contains any duplicates. 
        The naive algorithm checks all \(O(n^2)\) pairs. By first sorting in \(O(n\log n)\) 
        and then scanning adjacent elements in \(O(n)\), we can decide uniqueness in \(O(n\log n)\),
         a substantial improvement for large inputs.
      </p>
    </div>
    
    
<div class="example-box">
  <strong class="example-title">Example 2: Horner's Rule</strong>
  <p>
  Horner's Rule is a clever method of solving the 
  <a href="?path=Problems/Foundational/Polynomial Evaluation">Polynomial Evaluation</a> problem
  by rewriting a degree-<em>n</em> polynomial
    \[
      P(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n
    \]
    into the nested form
    \[
      P(x) = a_0 + x\bigl(a_1 + x(a_2 + \cdots + x(a_{n-1} + x\,a_n)\cdots)\bigr).
    \] 
    For instance, consider the 6th-degree polynomial
    \[
      P(x) = 1 + 2x + 3x^2 + 4x^3 + 5x^4 + 6x^5 + 7x^6,
    \]
    which Horner's Rule rewrites as
    \[
      P(x) = 1 + x\bigl(2 + x\bigl(3 + x\bigl(4 + x\bigl(5 + x\bigl(6 + x\cdot7\bigr)\bigr)\bigr)\bigr)\bigr).
    \]
  </p>
  <p>
  Although Horner's nested form can look mysterious, it directly yields the most efficient way to evaluate a polynomial. By reframing the usual sum-of-powers view, we expose a simple loop that does exactly one multiplication and one addition per coefficient. 
  Next we will look at the algorithm in action. 
  The demonstration below first shows step-by-step how to factor out the \(x\) terms in the non-obvious way
  you saw above.
  This is not really part of the algorithm, but it helps to show what the algorithm is doing and why it works.
  Then it walks through the steps of tha actual algorithm, showing how the steps match up with the factored form
  of the polynomial.
</p>

  <div class="embeddedDemoContainer">
    <iframe
      class="embeddedDemo"
      src="/Algorithms/Content/Demos/Transform-and-Conquer/Horners Rule Demo.html"
      allow="fullscreen"
      name="Horners-Rule-demo"
    ></iframe>
  </div>
  <p>The algorithm is surprisingly simple: It simply walks through the coefficients from high to low,
  multiplying the current result by \(x\) and then adding the nexty coefficient. 
  </p>
  <pre><code>hornersRule(a[0..n], x):
    result = a[n]
    for i from n-1 down to 0:
        result = a[i] + x * result
    return result  </code></pre>
    <p>Horner's Rule eliminates the need for explicit power computations and performs exactly one 
    multiplication and one addition per coefficient in a single loop, for a total of \(n\) 
    multiplications and \(n\) additions. Therefore, it runs in \(O(n)\) time and uses \(O(1)\) 
    extra space (ignoring input storage).</p>
    <p>The naive approach computes each term \(a_i x^i\) by performing \(i\) multiplications, 
    yielding \(O(n^2)\) operations overall. The straightforward linear-time method that accumulates powers 
    explicitly also runs in \(O(n)\) time but requires two multiplications per coefficient 
    (one to update the power variable and one to multiply by \(a_i\)), for a total of \(2n\) multiplications. 
    Horner's Rule, by contrast, uses only \(n\) multiplications&mdash;half as many&mdash;and \(n\) additions, 
    making it the most operation-efficient algorithm for polynomial evaluation. 
    Surprisingly, the algorithm for Horner's Rule is also simpler than the other two algorithms&mdash;even
    if coming up with it was more difficult.
    See the <a href="?path=Algorithms/Transform-and-Conquer/Horners Rule.html">Horner's Rule</a> 
    page for more details.</p>
  </div>

<div class="example-box">
  <strong class="example-title" id="heapsort-example">Example 3: Heapsort</strong>

  <p>
  Heapsort solves the 
  <a href="?path=Problems/Foundational/Sorting">Sorting</a> problem by transforming an array into a 
  heap (specifically, a max-heap).
    Recall that a heap is a complete binary tree stored in an array, 
    where each node is greater than or equal to its children (for a max-heap).  
    The primary heap operations needed by Heapsort are:
    <ul>
      <li><code>heapify</code>: Restore the heap property at a given node in \(O(\log n)\) time.
      It does so by "bubbling down" small elements to where they belong.</li>
      <li><code>buildMaxHeap</code>: Reorganize the elements of an array into a heap in \(O(n)\) time.
      Briefly, it walks through the tree backwards calling heapify on each node.</li>
    </ul>
    If you are unfamiliar with heaps, you should read the <a href="?path=Data Structures/Heaps">Heaps</a> page
    before continuing.
  </p>

  <p>
    Heapsort sorts an array in-place by first performing <code>buildMaxHeap</code>, 
    then repeatedly swapping the root (the maximum value) with the last element of the heap,
    reduce the heap size by one (so it no longer considers the maximum value as part of the heap),
    and calling <code>heapify</code> to restore the heap property on the smaller heap. 
    Ignoring the details of <code>buildMaxHeap</code> and <code>heapify</code>, 
    the algorithm is very simple:  
  </p>

  <pre><code>function heapSort(A):
    buildMaxHeap(A,n)     // Transform the array into a heap
    for i from n-1 down to 1:
        swap A[0] with A[i]     // move max to end
        heapify(A, 0, i-1)      // restore heap property
    return A  </code></pre>


  <p>See the Heapsort demo below for a step-by-step animation.</p>
  <div class="embeddedDemoContainer">
    <iframe
      class="embeddedDemo"
      src="/Algorithms/Content/Demos/Transform-and-Conquer/Heapsort Demo.html"
      allow="fullscreen"
      name="Heapsort-demo"
    ></iframe>
  </div>
  
  <p>
    <strong>Time Complexity:</strong> 
    Building the heap takes \(O(n)\), and each of the \(n-1\) extractions costs \(O(\log n)\)
    (constant time to swap, and \(O(\log n)\) time for <code>heapify</code>), 
    for a total of \(O(n + (n-1)\log n) = O(n\log n)\). 
    See the <a href="?path=Data Structures/Heaps">Heaps</a> page for a more detailed analysis of 
    <code>buildMaxHeap</code> and <code>heapify</code>.<br>
    <strong>Space Complexity:</strong> 
    Heapsort sorts in-place using only \(O(1)\) extra space.
  </p>
  <p>
  Although Heapsort is a clever use of the transform-and-conquer technique, 
  in practice Quicksort and Heap Sort outperform it.
  </p>
</div>
  </section>
  
<section id="algorithms">
  <h2>Algorithms Using This Technique</h2>
  <ul>
    <li>
      <strong>Heapsort</strong> - builds a max-heap in \(O(n)\) time, then repeatedly extracts the maximum in \(O(\log n)\) per extract, yielding an in-place \(O(n \log n)\) sort.
    </li>
    <li>
      <strong>Horner's Rule</strong> - rewrites a degree-\(n\) polynomial to avoid explicit powers, performing exactly one multiply and one add per coefficient for an \(O(n)\) evaluation.
    </li>
    <li>
      <strong>Fibonacci via Matrix Exponentiation</strong> - raises the \(2 \times 2\) Fibonacci matrix \(\begin{pmatrix}1 & 1 \\ 1 & 0\end{pmatrix}\) to the \(n\)th power in 
      \(O(n)\) time, or in \(O(\log n)\) time using fast exponentiation.
    </li>
        <li>
      <strong>Binary Exponentiation</strong> - computes \(x^n\) by recursively squaring and multiplying, reducing the cost from \(O(n)\) multiplications to \(O(\log n)\). Can be considered transform-and-conquer since it is based on thinking about the expontent's binary representation.
    </li>
    <li>
      <strong>Gaussian Elimination</strong> - transforms an \(n \times n\) system of linear equations into upper-triangular form using forward elimination, then solves by back-substitution in \(O(n^3)\) time.
    </li>
    <li>
      <strong>Binary Search Trees (BSTs)</strong> - transform array-based search into a tree structure by inserting elements into a binary tree where each node's left subtree contains smaller values and the right subtree larger; an in-order traversal reproduces the sorted array, and most operations run in \(O(h)\) time, where \(h\) is tree height.
    </li>
    <li>
      <strong>Balanced Search Trees</strong> - some are BST variants (e.g., AVL and red-black trees) that maintain height balance via rotations after insertions and deletions to guarantee \(O(\log n)\) search, insert, and delete; others are generalizations, such as 2-3 trees and B-trees, which use multi-way nodes to reduce tree height and maintain balance.
    </li>
  </ul>
</section>


<section id="when">
  <h2>When to Use</h2>
  <p>Transform-and-conquer shines in situations where a one-time change of form or structure unlocks asymptotic speedups or simplifies future operations. For instance:</p>
  <ul>
    <li>
      <strong>Preprocessing:</strong> When a preliminary transformation reduces the complexity of subsequent operations, such as pre-sorting data before searching or duplicate detection.
    </li>
    <li>
      <strong>Representation Change:</strong> When transforming the problem to a different form yields more efficient algorithms, for example using Horner's Rule to evaluate a polynomial in \(O(n)\) instead of \(O(n^2)\).
    </li>
    <li>
      <strong>Clever Data Structures:</strong> When storing the data in a different data structure
      allows efficient operations. For instance, constructing a heap in \(O(n)\) and 
      then performing many \(O(\log n)\) extract operations to sort.
    </li>
    <li>
      <strong>Repeated Queries:</strong> When solving a class of problems repeatedly on similar inputs justifies a one-time preprocessing cost, for instance building a balanced search tree to support fast lookups, insertions, and deletions in \(O(\log n)\).
    </li>    
    <li>
      <strong>Leveraging Existing Algorithms:</strong> When the problem matches a well-known transformation pattern&mdash;such as Gaussian elimination for linear systems, fast Fourier transform for polynomial convolution, or matrix exponentiation for recurrences&mdash;you can directly apply proven, optimized algorithms to ensure correctness and performance.
    </li>
  </ul>
</section>

<section id="limitations">
  <h2>Limitations</h2>
  <ul>
    <li>
      <strong>Transformation Overhead:</strong> The initial transformation or preprocessing step may carry significant cost, making the technique unsuitable when only a small number of operations follow.
    </li>
    <li>
      <strong>Input Dynamics:</strong> When data changes frequently, repeated transformations (e.g., rebuilding a tree after each update) can negate performance gains.
    </li>
    <li>
      <strong>Space Tradeoffs:</strong> Some transformations require additional memory (such as auxiliary arrays or recursion stacks), which may be impractical in space-constrained environments.
    </li>
    <li>
      <strong>Implementation Complexity:</strong> Transform-and-conquer algorithms can be more complex to implement and debug than straightforward methods, increasing development time and risk of errors.
    </li>
    <li>
      <strong>Problem Suitability:</strong> Not all problems lend themselves to efficient transformations; forcing a transformation may lead to convoluted or suboptimal solutions compared to simpler approaches.
    </li>
  </ul>
</section>
  
  <section id="tips">
  <h2>Implementation Tips</h2>
  <ul>
    <li>
      <strong>Separate transformation from core logic:</strong> Keep preprocessing or representation-change code modular so it can be tested and reused independently of the main algorithm.
    </li>
    <li>
      <strong>Use in-place operations when memory is limited:</strong> Favor algorithms that modify existing data structures (e.g., heapify in Heapsort) rather than allocating new arrays to reduce space overhead.
    </li>
        <li>
      <strong>Isolate structural invariants:</strong> Factor out data-structure maintenance&mdash;such as BST rotations and balance checks&mdash;into dedicated routines; for example, separate all AVL or red-black tree rebalancing logic so that insertion and deletion code remains clear and you can validate height or color invariants separately.
    </li>
    <li>
      <strong>Profile end-to-end performance:</strong> Measure both the cost of the transformation step and subsequent operations to confirm that the overall approach yields net speedups for your target input sizes.
    </li>
    <li>
      <strong>Leverage optimized libraries:</strong> For complex transforms (e.g., FFT, matrix multiplication), consider using well-tested library implementations to benefit from low-level optimizations and avoid subtle bugs.
    </li>
  </ul>
</section>

 <section id="pitfalls">
  <h2>Common Pitfalls</h2>
  <ul>
    <li>
      <strong>Misjudging transformation cost:</strong> Preprocessing or representation-change overhead may exceed benefits for small inputs or when only a few operations follow.
    </li>
    <li>
      <strong>Breaking problem invariants:</strong> Ensure transformations preserve the original semantics; for example, when converting an array into a heap, verify that all elements remain present and the heap property holds correctly before extraction steps.
    </li>
    <li>
      <strong>Edge-case oversight:</strong> Failing to handle trivial or degenerate inputs (empty arrays, zero-degree polynomials, odd-sized splits) can cause incorrect results or runtime failures.
    </li>
    <li>
      <strong>Numerical instability:</strong> Transforms like Gaussian elimination or FFT can introduce round-off errors if not implemented with stability considerations (pivoting, precision management).
    </li>
    <li>
      <strong>Over-engineering solutions:</strong> Applying complex transforms where simple direct algorithms suffice increases code complexity and reduces maintainability without clear performance gains.
    </li>
    <li>
      <strong>Spurious side-effects:</strong> In-place transformations may corrupt original data if copies are not made when needed later, such as modifying the coefficient matrix in Gaussian elimination and losing the original system of equations.
    </li>
  </ul>
</section>

<section id="applications">
  <h2>Real-World Applications</h2>
  <ul>
    <li>
      <strong>Database Indexing:</strong> Balanced B-trees and 2-3 trees transform flat datasets into multi-way search structures, enabling efficient range queries and updates in \(O(\log n)\) time&mdash;crucial for high-performance database systems.
    </li>
    <li>
      <strong>Computer Graphics:</strong> Horner's Rule efficiently evaluates polynomial curves and shading functions per pixel in \(O(n)\) time, enabling real-time rendering with minimal computational overhead.
    </li>
    <li>
      <strong>Scientific Simulations:</strong> Gaussian elimination transforms sparse system matrices into triangular form for reliable, direct solutions in finite-element analyses and computational fluid dynamics.
    </li>
    <li>
      <strong>Task Scheduling:</strong> Pre-sorting jobs by deadlines or priorities restructures scheduling problems, allowing greedy or dynamic programming algorithms to optimize throughput, minimize lateness, or balance resource utilization.
    </li>
  </ul>
</section>


<section id="summary">
  <h2>Summary & Key Takeaways</h2>
  <p>
    Transform-and-conquer techniques apply one-time transformations&mdash;such as preprocessing, representation changes, and subproblem reduction&mdash;to expose more efficient algorithms than direct methods. This approach is most valuable when a transformation unlocks faster operations across many queries, when mapping to specialized data structures or mathematical forms yields asymptotic speed-ups, or when established algorithmic templates (e.g., FFT, Gaussian elimination) can be leveraged. Always weigh the cost of transformation against the overall performance benefits, verify that problem invariants and numerical stability are maintained, and choose this technique when it clearly simplifies or accelerates the target problem.
  </p>
</section>

<section id="resources">
  <h2>Related Links and Resources</h2>
  <ul>
    </li>
    <li>
      <a href="https://www.geeksforgeeks.org/transform-and-conquer-technique/">GeeksforGeeks: Transform-and-Conquer Technique</a> Detailed tutorial covering instance simplification, representation change, and problem reduction with code examples.
    </li>
    <li>
      <a href="https://csc.lsu.edu/~jianhua/ch06n.pdf">LSU Lecture Notes: Chapter 6 Transform-and-Conquer</a> Lecture slides that accompany Levitin's examples on presorting, median finding, and heap building.
    </li>
    <li>
      <a href="https://www.csl.mtu.edu/cs4321/www/Lectures/Lecture%2012%20-%20Transform%20and%20Conquer-Presort%20and%20Heap.htm">MTU CS Lecture: Transform and Conquer-Presort and Heap</a> Example-driven notes on instance simplification and representation change strategies. 
    </li>
    <li>
      <a href="https://medium.com/@cokasaki/algorithm-design-technique-transform-conquer-e23a43b3b1b3">Medium: Algorithm Design Technique - Transform & Conquer</a> Conceptual discussion and insights on structuring multi-stage algorithm designs.
    </li>
  </ul>
</section>

  
 <section id="reading-questions">
  <h2>Reading Comprehension Questions</h2>
  <ol>
    <li>Name the three main categories of transform-and-conquer techniques, and give one example of each.</li>
    <li>How does representation change differ from instance simplification?</li>
    <li>Why must you consider preprocessing cost versus query efficiency when using transform-and-conquer?</li>
    <li>Describe one common pitfall specific to transform-and-conquer and explain how to avoid it.</li>
    <li>
      Explain why Horner's Rule is a transform-and-conquer technique. 
      How does rewriting a degree-\(n\) polynomial into nested form reduce the evaluation cost 
      from \(O(n^2)\) to \(O(n)\)?
    </li>
    <li>
      Rewrite the polynomial  
      \[
        P(x) = 7x^4 - 5x^3 + 3x^2 - 2x + 1
      \]
      in Horner's form.
    </li>
    <li>
      Describe how Heapsort transforms an unsorted array into a max-heap in \(O(n)\) time. 
      Why does this transformation enable each subsequent swap/heapfify operation to run in \(O(\log n)\) time each?
    </li>
    <li>
      In the transform-and-conquer taxonomy, Heapsort uses which type of transformation
      (instance simplification, representation change, or problem reduction)? Justify your answer.
    </li>

  </ol>
  <button id="toggleAnswers" class="show-answer" aria-expanded="false">Show Answers</button>
  <div id="answers" class="answer" hidden>
    <ol>
      <li><strong>Answer:</strong> Instance simplification (e.g. presorting), representation change (e.g. Horner's Rule), problem reduction (e.g. reducing matching to max-flow).</li>
      <li><strong>Answer:</strong> Representation change alters data encoding or structure (e.g. array -> heap); instance simplification reduces size/complexity via preprocessing (e.g. presorting).</li>
      <li><strong>Answer:</strong> The one-time transformation may be costly; ensure subsequent speedups outweigh that initial overhead for your input sizes.</li>
      <li><strong>Answer:</strong> A common pitfall is misjudging transformation cost; avoid it by profiling both preprocessing and follow-up operations to confirm net gains.</li>
       <li>
      <strong>Answer:</strong> Horner's Rule is transform-and-conquer because it changes the representation of the polynomial from a standard sum of powers into a nested form. This one-time transformation replaces repeated exponentiation (which would cost \(O(n^2)\) if you computed each \(x^k\) separately) with a single pass that does exactly one multiplication and one addition per coefficient, for a total of \(O(n)\).
    </li>
    <li>
      <strong>Answer:</strong>  
      \[
        P(x) = 7x^4 - 5x^3 + 3x^2 - 2x + 1
        \;\longrightarrow\;
        1+ x(-2 + x(3+ x(-5 + 7x)))
      \]
    </li><li>
    <strong>Answer:</strong> 
    Heapsort first performs a bottom-up heapify on the array: it runs heapify on every node from index
    \(\lfloor n/2\rfloor -1\) down to \(0\).  
    Although a single heapify can take \(O(\log n)\), the total cost of all these operations 
    turns out to be  \(O(n)\) (the reason why was left as an exercise).  
    This builds a valid max-heap in place.  Each swaps takes constant time, 
    and the call to heapify on the new root takes \(O(\log n)\) time since the height of the heap is 
    \(\lfloor\log n\rfloor\).
  </li>
    <li>
      <strong>Answer:</strong> Heapsort uses <em>representation change</em>&mdash;it turns the flat array into a heap-structured tree. By changing representation, the subsequent extract-max and insert operations become more efficient.
    </li>
    </ol>
  </div>
</section>

<section id="activities">
  <h2>In-Class Activities</h2>
  <ol>
    <li><strong>BuildHeap Variations:</strong>
        Compare and contrast the following variations of <code>buildHeap</code>. 
        Think in terms of both time and space.
      <ol type='a'>
      <li>The one presented above.</li>
      <li>One that inserts each item from the array into an initially empty heap one by one.</li>
      </ol>
    </li>
    <li><strong>Heapsort Variations:</strong>
      Compare and contrast the following variations of Heapsort.
        Think in terms of both time and space.
      <ol type='a'>
      <li>The one presented above.</li>
      <li>One that replaces the <em>swap-then-heapify</em> step with <em>extractMin-and-copy-to-end</em>.</li>
      </ol>
    </li>
    <li><strong>Heapsort versus Others:</strong>
    It was suggested that in general Heapsort does not perform as well as Quicksort and Merge Sort. 
    Discuss reasons you think this may be the case.
    </li>
    <li><strong>Presorting Race:</strong> Split the class in groups. Each group is given a set of cards (playing cards 
    or cards with numbers on them&mdash;16 or so is probably a good number) 
    that they should lay out in a line randomly (to resemble an array). 
    Each group needs to perform a given number of search queries (6-8 might be good).
    Half of the groups should first sort their list and then perform the searches.
    The other half should perform the searches on their unsorted list.
    After doing this, the class should discuss which method was better/more efficient.
    A good variation might be to count the number of operations performed to get a more accurate comparision.
    For linear/binary search it is pretty easy to count. For sorting, use number of times they compare cards and/or
    swap cards. They can use whatever algorithm they like. 
    They do not have to be precise with the counts as long as it is reasonably close.
    </li>
    <li><strong>Polynomial Evaluation Comparison:</strong> Have groups of 2 or 3 students work on the board to
    evaluate the polynomial \(P(x) = 3x^5 - 8x^4 + 2x^3 +9x^2 - 2x + 17\) at \(x=3\). Assign each group one
    of the following algorithms:
      <ol type='a'>
      <li>Naive \(O(n^2)\) algorithm (computes \(x^i\) from scratch at each iteration).</li>
      <li>Better \(O(n)\) algorithm (keeps track of \(result\) and \(power\) so it can compute 
      \(x^{i}\) as \(x\cdot x^{i-1}\) in one multipliation using \(power=power\cdot x\)).</li>
      <li>Horner's Rule algorithm</li>
      </ol>
      Count the number of additions and multiplications used for each and compare.
    </li>
    <li><strong>Horner's Rule Workshop:</strong> Rewrite given polynomials into nested form, then simulate the evaluation loop step by step.</li>
    <li><strong>Transformation Brainstorm:</strong> In small groups, pick a common problem (e.g. string matching) and propose a transform-and-conquer approach; share your plan with the class.</li>
  </ol>
</section>

<section id="problems">
  <h2>Homework Problems</h2>
  <h3>Basic</h3>
  <ol>
    <li>Use Heapsort to sort the following array. Show the array after <code>BuildHeap</code>, and after every iteration
    of the loop. \(A=[J, G, E, K, B, F, H, D, I, A, C]\).</li>
    <Li>Given the array \(A=[3, 6, 8, 1, 5, 7, 2, 4]\), sort it using Heapsort, Quicksort and Merge Sort. For each,
    count the total number of comparisions, swaps, and moves/copies. 
    Compare the three algorithms based on the results.
    How do you think the results will scale with the number of elements of an array?</li>
    <li>Use Horner's Rule to evaluate the polynomial \(P(x) = 4 + 3x + 2x^2 + x^3\) at \(x = 5\). Show each step and count the number of multiplications and additions.</li>
     <li>If you need to search for a value in an unsorted array with \(n\) elements \(m\) times,
    for
    what values of \(m\) would it make sense to just use linear search, and for what values of \(m\) would
    it make sense to sort the array first and then use binary search? Clearly justify your answers.</li>
  </ol>
  <h3>Advanced</h3>
  <ol>
    <li>Write a function that performs Gaussian elimination with partial pivoting on an \(n \times n\) system. Analyze both its time complexity and numerical stability considerations.</li>
    <li>The <em>mode</em> of an array is the value that occurs the most times in the array.
    Come up with as many different algorithms as you can to determine the mode of an array.
    Specify the design technique each uses, and give the time/space complexity. 
    Then choose the best and explain why it is the best.
  </ol>
</section>

  
</body>
</html>
