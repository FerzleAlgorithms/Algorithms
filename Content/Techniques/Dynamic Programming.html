<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Technique: Dynamic Programming</title>
  <script src="/Algorithms/scripts/chapterScripts.js"></script>
  <link rel="stylesheet" href="/Algorithms/css/style.css">
  <link rel="stylesheet" href="/Algorithms/css/chapter.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <h1>Dynamic Programming</h1>

  <!-- Motivation & Introduction -->
  <section id="introduction">
    <h2>Introduction</h2>
    <p>
      Dynamic programming (DP) is a powerful technique for solving optimization and counting problems by 
      breaking them into overlapping subproblems and storing (memoizing) their solutions (often in a table/array).  It relies on two 
      key properties:
    </p>
    <ul class="spaced">
      <li><strong>Optimal substructure:</strong> an optimal solution to the whole can be composed from optimal solutions to subproblems.</li>
      <li><strong>Overlapping subproblems:</strong> the same subproblems recur multiple times.</li>
    </ul>
      <p>
    These two properties are necessary because they allow us to build complex solutions efficiently. 
    
    <ul class="spaced">
      <li>
      Optimal substructure means that solving larger instances can be reduced to combining solutions of 
    smaller instances without losing correctness.  In particular, it is what allows us to construct
    a recursive formula that is the basis of the technique.</li>
      <li>
      Overlapping subproblems ensures that many of these smaller instances repeat, so by storing (or tabulating) 
    their results once, we avoid redundant work and achieve polynomial-time performance instead of exponential.
    Technically, a problem does not need to have overlappins subproblems to be solved by dynamic programming,
    but there is no benefit of the approach in this case.</li>
    </ul>
  </p>   
  <p>
  Because dynamic programming uses a table to store results, it can be considered a special case of space-time tradeoffs.
  </p>
    <p>
      Classic first examples include the coin row problem, where you pick non-adjacent coins for maximum value, 
      and computing Fibonacci numbers without exponential blow-up.  We'll see both bottom-up (tabulation) 
      and top-down (memoized recursion) approaches in action. Later, hopefully you will also read about dynamic
      programming algorithms to solve the 0-1 Knapsack problem, Matrix Chain Multiplication, Longest Common Subsequence,
      and others.
    </p>
  </section>



<section id="examples">
  <h2>Examples</h2>
  
  <div class="example-box">
  
  <strong class="example-title">Example: Coin Row Problem</strong>
    <p>
      In the Coin Row Problem, you are given a row of coins with values \(c_1, c_2, \dots, c_n\), 
      and the goal is to  choose a subset of non-adjacent coins to maximize the total value.
      That is, you cannot choose two coins that are next to each other.
    </p> 
    <p>
    For example, given the set of values \([1, 6, 8, 7]\), it is not hard to see that taking coins 6 and 7
    will give the maximum value of \(6+7=13\).
  </p>
  
    <p>
    The dynamic programming approach works as follows.
    Index the coins from \(0\) to \(n-1\), with values \(C[0],\) \(C[1],\) \(\dots,\) \(C[n-1]\).
    Define an array \(M\) of length \(n\) so that \(M[j]\) is the maximum
    value obtainable for coins \(0\) through \(j\).  
    We initialize the first two entries directly,
    then for each subsequent coin choose between skipping it or taking it (in which case
    we add its value to the best solution two steps back).
  </p>
  <p>In words:</p>
  <ul>
    <li>\(M[0] = C[0]\) &nbsp; &nbsp; (only coin 0 is available).</li>
    <li>\(M[1] = \max\{ C[0], C[1] \}\) &nbsp; &nbsp; (best of coin 0 or coin 1).</li>
    <li>For each \(j = 2,3,…,n-1\), there are two choices:
      <ul>
        <li>Skip coin \(j\): The best we can do is the solution for coins \(1\) through \(j-1\), giving value \(M[j-1]\).</li>
        <li>Take coin \(j\): The best we can so it take coin \(j\) and add it to the best solution 
        for coins \(0\) through \(j-2\) (since we cannot take coin \(j-1\)), giving value \(M[j-2] + C[j]\).</li>
      </ul>
      Thus, each \(j = 2,3,…,n-1\),
      \[M[j] = \max\{ M[j-1], M[j-2] + C[j] \}.\]
    </li>
  </ul>
  <p>
    The optimal value obtainable for all \(n\) coins is \(M[n-1]\).  
    Putting this together, we get the following pseudocode.
  </p>
  <pre><code>coinRowDP(C, n):
    if n == 0:
        return 0
    if n == 1:
        return C[0]
    M = [n] // An array of size n
    M[0] = C[0]
    M[1] = max(C[0], C[1])
    for j from 2 to n-1:
        M[j] = max(M[j-1], M[j-2] + C[j])
    return M[n-1]
  </code></pre>
  
<!-- Correctness argument -->
<p>
  In order to prove that the algorithm returns the correct answer, 
  we prove by induction on \(j\) that \(M[j]\) indeed stores the maximum value obtainable from 
  coins \(0\) through \(j\). 
  For the base cases \(j=0\) and \(j=1\), the formula clearly gives the optimal solution.  
  Assume it holds for all indices up to \(j-1\).  Now consider an optimal solution for coins \(0\) through \(j\).  
  If coin \(j\) is not in the solution, then total value is exactly the optimum for coins 
  up to \(j-1\), namely \(M[j-1]\).
  If coin \(j\) is taken, the solution cannot include coin \(j-1\), 
  so its value is \(C[j]\) plus the optimum for coins up to \(j-2\), namely \(M[j-2] + C[j]\).  
  Our recurrence \(M[j] = \max\{M[j-1],\,M[j-2] + C[j]\}\) therefore captures both possibilities and thus yields the optimal value.
</p>

    <p>
    The following demonstration shows the algorithm in action, including the part we have not discussed yet:
    determining which coins to take. We will discuss that after the demo.
    </p>
    <div class="embeddedDemoContainer">
      <iframe class="embeddedDemo" 
      src="/Algorithms/Content/Demos/Dynamic Programming/Coin Row Demo.html"></iframe>
    </div>
      
       <p>
    As seen in the demo, 
    once the array \(M\) is filled, we can determine which coins yield the optimal value by scanning
    backward from index \(n-1\) and comparing \(M[j]\) with \(M[j-1]\). 
    If you think back to how the first part of the algorithm works, it is not hard to see that
    <ul>
    <li>If \(M[j] > M[j-1]\), then coin \(j\) was definitely taken.</li>
    <li>If \(M[j] = M[j-1]\), we can assume coin \(j\) was not taken.
    (If we want to find all solutions, we would have to do an additional check for this case since it
    might lead to two different solutions, but we will not worry about that here.)</li>
    </ul>
    If coin \(j\) was taken, coin \(j-1\) was not, so we consider coin \(j-2\).
    If coin \(j\) was not taken, then we consider coin \(j-1\). 
    We also need a base case: If we ever consider coin \(0\), then we take it since there is no
    reason not to.
    This leads to the following pseudocode.
  </p>
  <pre><code>determineCoins(C, M, n):
    S = []    // Selected coins array
    j = n - 1
    while j &gt;= 0:
        if j == 0 or M[j] > M[j - 1]:
            S.add(C[j])
            j = j - 2
        else:
            j = j - 1
    return S</code></pre>
      <p>
      <strong>Time Complexity:</strong> It is pretty clear that the complexity of both parts of 
      the algorithm is \(O(n)\).<br>
      <strong>Space Complexity:</strong> It requires \(O(n)\) space for the arrays \(M\) and \(S\), plus a constant
      number of other variables. 
      Finding the maximal value can be optimized to \(O(1)\) space by only keeping two variables, 
      but then determining the coins becomes impossible (or very tricky&mdash;I do not know how to make it work).
      </p>
      <!-- Pattern comment -->
    <p>
      The idea of strategically deciding for each item whether to take it or skip it underlies many algorithms. 
      This pattern shows up 
      not only in dynamic programming algorithms to solve various other problems (e.g. this one, 0-1 Knapsack), 
      but also in techniques like backtracking and branch-and-bound. 
    </p>
  </div>  
  
   <p>
    You may have seen that the brute force algorithm to compute the \(n\)th Fibonacci number 
    (See the
    <a class="problem" href="?path=Problems/Other/Fibonacci">Fibonacci number</a> page
    if you are not already familiar)  
    is terrible
    (If not, read about it on the 
    <a class="problem" href="?path=Techniques/Brute%20Force">Brute Force</a> page). 
    Dynamic Programming is exactly what we need to fix that algorithm. 
    Instead of recomputing the same Fibonacci numbers repeatedly, we can compute them once
    and store the result. Then we can look it up when we need it again.
    </p>
    <p>
    There are two ways to implement this idea:
    <ul>
    <li><strong>Top-Down (sometimes called Memoization):</strong> 
    Start with the naive recursive algorithm, but every time 
    a value is computed, store it in an array. 
    Then, before a recursive call is made, the algorithm first looks at the array to see if it
    has already been computed. If so, use that value. If not, make a recursive call.
    </li>
    <li><strong>Bottom-Up (sometimes called Tabulation):</strong>
    Start by solving the smallest cases of the problem, itertatively building your way to the final solution.
    </li>
    </ul>
    </p>
    <p>
    These two approaches are applicable to all dynamic programming algorithms.
    However, there are important differences. 
    In cases where determining the optimal solution requires knowing the solution to <em>every</em>
    subproblem, bottom-up is generally preferred since by design it computes solutions to every
    subproblem, and it avoids the overhead of recursion.
    On the other hand, for some problems, not every subproblem needs to be solved. 
    In these cases, significant time can be saved by using the top-down approach since it only 
    computes the values needed by the initial call.
    </p>
    <p>
    We will look at both of these approaches to computing the \(n\)th Fibonacci number.
    </p>
    
  <div class="example-box">
    <strong class="example-title">Example 2: Fibonacci Numbers (Top-Down)</strong>
     <p>
        To compute the \(n\)th Fibonacci number using the top-down dynamic programming algorithm,
        we need to create an array to store the first \(n\) Fibonacci numbers,
        initialize it with some value to indicate the values have not been computed yet (we will use 0),
        and modify the brute force algorithm to store and lookup the values as needed.
        Here is the pseudocode.
      </p>
      <pre><code>fibonacciTopDown(n)
  fib = [0...0];          // Array of n 0s
  return fibRec(n, fib);  // Start the recursion

fibRec(i, fib) { // Recursive helper with memoization //
   if (i < 2)             // Base cases
      fib[i]=i
      return fib[i]
   else if (fib[i] != 0)  // If we've already computed fib[i], return it
       return fib[i]
   else                   // Otherwise compute, store, and return
       fib[i] = fibRec(i - 1, fib) + fibRec(i - 2, fib)
       return fib[i]</code></pre>

    <p>Here is a demonstration of the algorithm.
    </p>
      <div class="embeddedDemoContainer">
        <iframe class="embeddedDemo" 
        src="/Algorithms/Content/Demos/Dynamic Programming/Fibonacci Top Down Demo.html"></iframe>
      </div>
      
       <p>
      <strong>Time Complexity:</strong> 
      Although we could simply say something like "It is clear that this algorithm runs in \(O(n)\) time,"
      and many readers would just buy it and/or move on and accept it,
      we will instead go overboard to prove it by induction since reviewing induction might not be a bad thing.</p>
      <p> 
      <code>fibonacciTopDown(n)</code> initializes an array of size \(n\) (which takes either constant time or
      \(O(n)\) time depending on implementation) and makes a call to <code>fibRec(n,fib)</code>.
      Thus, if we can prove that <code>fibRec(n,fib)</code> is linear, this implies that <code>fibonacciTopDown(n)</code>
      is as well.</p>
      <p>
      First, notice that <code>fibRec(i,fib)</code> executes a constant number of instructions aside
      from the recursive calls. Let \(C\) be the maximum number of instructructions that 
      <code>fibRec(i,fib)</code> executes for any path through its execution (not including the recursive calls, if any).
      We will use induction to prove that for all \(k\geq 1\), 
      <code>fibRec(k,fib)</code> executes at most \(2C\, k\) instructions,
      and fills entries 0 through \(k\) of the \(fib\) array (For technical reasons, we have to exclude \(k=0\). Do you see why?).
      <div class="lemma">
          <p>For all \(k\geq 1\), 
              <code>fibRec(k,fib)</code> executes at most \(2C\, k\) instructions,
              correctly fills entries 0 through \(k\) of the \(fib\) array, 
              and returns \(F_k\), the \(k\)th Fibonacci number.</p>
              <p><strong>Proof:</strong><br>
          <strong>Base case:</strong> 
            When \(k = 1\), the algorithm uses at most \(C\leq 2C\,1\) steps, sets \(fib[1]=1\),
            and by default, \(fib[0]=0\) already. It then returns \(F_1=1\).</br>
          <strong>Induction hypothesis:</strong> 
            Assume that for all \(1\leq j < k\), the call <code>fibRec(j, fib)</code> takes no more that \(2C\, j\) steps,
            correctly fills entries 0 through \(j\) of the \(fib\) array, and returns \(F_j\).<br>
          <strong>Induction step:</strong> 
            Let \(k > 1\). The call to <code>fibRec(k, fib)</code> takes at most \(C\) steps, plus the cost of
            the two recursive calls. By the induction hypothesis, the call to <code>fibRec(k-1, fib)</code> takes at
            most \(2C(k-1)\) steps, correclty fills entries 0 through \(k-1\) of the \(fib\) array, and returns \(F_{k-1}\).
            Thus, when <code>fibRec(k-2, fib)</code> is called, entry \(fib(k-2)\) is filled, 
            so it takes at most \(C\) steps and returns \(F_{k-2}\) without a recursive call.
            Finally, according to the code, <code>fibRec(k, fib)</code> correctly returns \(F_k=F_{k-1}+F_{k-2}\),
            and does so in no more than \(C+2C(k-1)+C\)\(=\)\(2C\, k\) steps.
          </p>
     </div>
    <p>
    Hopefully you can put it all together and see that <code>fibonacciTopDown(n)</code> has a time complexity of \(O(n)\).
    </p>
  <strong>Space Complexity:</strong> It requires \(O(n)\) space for the \(fib\) array, plus a few extra local variables.
      </p>
    </div>
    

  <div class="example-box">
    <strong class="example-title">Example 3: Fibonacci Numbers (Bottom-Up)</strong>
      <p>
        Computing the \(n\)th Fibonacci number by filling an array from the bottom up
        turns out to be a lot easier than the top-down approach: Compute \(F_0=0\) and \(F_1=1\) to start,
        then just use the definition \(\left(F_n=F_{n-1}+F_{n-2}\right)\) to compute \(F_2\), \(F_3\), etc. until you get 
        to \(F_n\). The pseudocode is almost trivial:
      </p>
      <pre><code>fibonacciBottomUp(n):
  F = [n] // array of size n
  F[0] = 0
  F[1] = 1
  for i from 2 to n:
    F[i] = F[i-1] + F[i-2]
  return F[n]
</code></pre>
<p><strong>Time Complexity:</strong> It is not too difficult to see that this takes \(O(n)\) time.
</p>
<p><strong>Space Complexity:</strong> Clearly it takes \(O(n)\) memory: \(n\) for the array and a few local variables.
    </p>
    <p>
    Here is a demo of the algorithms.
    </p>
      <div class="embeddedDemoContainer"><iframe class="embeddedDemo" src="/Algorithms/Content/Demos/Dynamic Programming/Fibonacci Bottom Up Demo.html"></iframe>
      </div>
    <p>Clearly the bottom-up approach is better in every way than the top-down in this case!
    We can do even better by making the space requirement only \(O(1)\). More on that later.</p>
    <p>I feel kind of bad that this example is so much shorter than the other ones.
    But when you use the right technique in the right way, sometimes it's just that easy!</p>
    </div>
    
    <p>
    These examples are pretty simple compared to most instanced of dynamic programming (which is why we presnted them first!). 
    For many problems, a 2-dimensional array is needed to store values. But the idea is the same: Define a recurrence relation that
    defines a solution (or cost of a solution) based on subsolutions, use an array to keep track of subsolutions, and
    choose top-down or bottom-up.</p>
    <p> We will also see that for some optimization problems, a second array is needed in order to
    determine the solution&mdash;the main array used in dynamic programming algorithms for optimization problems only stores
    information about the<em>cost</em> or <em>value</em> of the subsolutions. In some cases, that array provided enough 
    information to allow us to compute the solution (e.g. coin-row (as we saw), 0-1 knpasack problem). 
    But in other cases, it does not (as we will see with matrix chain multiplication).
    </p>
  </section>
  
<section id="algorithms">
  <h2>Algorithms Using This Technique</h2>
  The following are dynamic programming algorithms or problems where a dynamic programming algorithm exists, but does not have an official name, so we just list the name of the problem.
  <ul>
    <li><strong>0-1 Knapsack Problem:</strong> Choose items with weights \(w_i\) and values \(v_i\) to maximize \(\sum_{i=1}^n v_i\) subject to \(\sum_{i=1}^n w_i \le W\).</li>
    <li><strong>Matrix Chain Multiplication:</strong> Determine optimal parenthesization of matrices \(A_1 \cdots A_n\) to minimize total scalar multiplications.</li>
    <li><strong>Optimal Binary Search Tree:</strong> Build a BST for keys with given search probabilities to minimize expected search cost.</li>
    <li><strong>Floyd–Warshall Algorithm:</strong> Compute all-pairs shortest paths in a weighted graph by iteratively improving path lengths.</li>
    <li><strong>Bellman–Ford Algorithm:</strong> Find shortest paths from a single source in graphs that may have negative edge weights.</li>
    <li><strong>Coin Change Problem:</strong> Count the number of ways (or minimize the number of coins) to make amount \(n\) using given coin denominations.</li>
    <li><strong>Edit Distance:</strong> Compute the minimum number of insertions, deletions, and substitutions to transform one string into another.</li>
    <li><strong>Longest Common Subsequence (LCS):</strong> Find the longest sequence common to two strings \(X\) and \(Y\).</li>
    <li><strong>Longest Increasing Subsequence (LIS):</strong> Find the longest strictly increasing subsequence in a sequence of numbers.</li>
    <li><strong>Subset Sum Problem:</strong> Decide whether a subset of \(\{a_1, \dots, a_n\}\) sums exactly to a target \(T\).</li>
  </ul>
</section>

<section id="when">
  <h2>When to Use</h2>
  <ul>
    <li><strong>Overlapping Subproblems:</strong> Identical subproblems recur and can be computed once and reused.</li>
    <li><strong>Optimal Substructure:</strong> On optimal solution can be built from optimal solutions to its subproblems.</li>
    <li><strong>Recurrence Relation:</strong> You can express the solution as a recurrence combining smaller subproblem results.</li>
    <li><strong>Polynomial-Size State Space:</strong> The total number of distinct subproblems is bounded by a polynomial in the input size.</li>
    <li><strong>Efficient Computations:</strong> Each table entry is filled by choosing among a small number of options (preferrably a small constant (often 2), sometimes linear), so the overall dynamic programming algorithm runs in polynomial time.</li>
    <li><strong>Exponential Naïve Recursion:</strong> A straightforward recursive approach runs in exponential time, but dynamic programming reduces it to polynomial time.</li>
  </ul>
</section>

<section id="Limitations">
  <h2>Limitations</h2>
  <ul>
    <li><strong>High Memory Usage:</strong> For some problems, the lookup table can be fairly large. For instance, the table for the 0-1 knapsack  (which you will hopefully
    read about later) has size \(n\times W\), where \(n\) is the number of items, and \(W\) is the size of the knapsack. Although this sounds fine, 
    what if \(W=2^n\)? Not only does this take a lot of space, but filling it takes a lot of time!</li>
    <li><strong>State Space Explosion:</strong> If the number of distinct subproblems grows exponentially with input parameters, dynamic programming becomes infeasible.</li>
    <li><strong>Complex Recurrence Derivation:</strong> Formulating the correct recurrence relation and transition logic can be nontrivial and error-prone.</li>
    <li><strong>Poor Parallelism:</strong> Dependencies between table entries often limit opportunities for parallel or distributed computation.</li>
    <li><strong>Discrete Substructure Required:</strong> Problems without clear, finite subproblem structure (e.g. continuous problems) cannot benefit from dynamic programming.</li>
  </ul>
</section>

<section id="tips">
  <h2>Implementation Tips</h2>
  <ul>
    <li><strong>Choose Bottom-Up vs. Top-Down:</strong> Bottom-up tabulation often yields clearer, faster loops when you need to compute all subproblems; Top-down memoization is more efficient if only a subset of subproblems is required, and is quicker to write and easier to debug.</li>
    <li><strong>Initialize Base Cases Explicitly:</strong> Set all base-case entries (e.g. first row and/or column of table) before filling the rest to avoid uninitialized values.</li>
    <li><strong>Optimize Space:</strong> If your recurrence only uses the previous row or a fixed window, use rolling arrays to drop memory from \(O(nm)\) to \(O(m)\) or \(O(n)\). 
    This will not work in some cases, e.g. where the entire table is needed to determine the details of the optimal solution and not just the cost.</li>
    <li><strong>Order Loops to Satisfy Dependencies:</strong> Ensure your outer and inner loops follow the natural dependency order (e.g. rows then columns or vice-versa, depending on the recurrence relation) so every subproblem is ready when needed.</li>
    <li><strong>Use Suitable Data Types:</strong> Choose integer or floating types (e.g. 64-bit) that can hold your dynamic programming values without overflow or precision loss.</li>
    <li><strong>Encapsulate Transitions:</strong> Factor out the recurrence logic into a helper function or macro to reduce code duplication and simplify testing.</li>
    <li><strong>Test, including Edge Cases:</strong> Verify correctness and expected efficiency on small inputs, 
    including minimal and maximal inputs (e.g. 0-1 knapsack with only 1 item, or with a bag so heavy it fits all items, or so light
    it fits none).</li>
  </ul>
</section>

<section id="pitfalls">
  <h2>Common Pitfalls</h2>
  <ul>
    <li><strong>Off-by-One Errors:</strong> Loop indices and table dimensions often mismatch; ensure your loops cover exactly the intended range.</li>
    <li><strong>Incorrect Base Cases:</strong> Failing to initialize or using wrong values for base cases causes cascading errors in the table.</li>
    <li><strong>Wrong Dependency Order:</strong> Filling entries before their prerequisites can lead to use of uninitialized values; order your loops to satisfy dependencies.</li>
    <li><strong>Excessive Memory Usage:</strong> Very large tables can exhaust memory—consider rolling array when only a subset of data is needed.</li>
    <li><strong>Stack Overflow:</strong> Deep recursion in top-down memoization can exceed call-stack limits on large inputs; switch to bottom-up if needed.</li>
    <li><strong>Incorrect Recurrence Logic:</strong> A small typo or off-by-one in the transition formula can produce completely wrong results.</li>
  </ul>
</section>

<section id="applications">
  <h2>Real-World Applications</h2>
  <ul>
    <li><strong>Network Routing:</strong> Computing shortest or least-cost paths in large-scale networks via dynamic programming on path costs (e.g. Bellman–Ford in distance-vector routing).</li>
    <li><strong>Job Scheduling:</strong> Selecting and sequencing jobs to maximize profit or minimize makespan using dynamic programming algorithms like weighted interval scheduling.</li>
    <li><strong>Sequence Alignment:</strong> Aligning DNA, RNA, or protein sequences using the Needleman–Wunsch or Smith–Waterman dynamic programming algorithms.</li>
    <li><strong>Viterbi Decoding:</strong> Finding the most probable state sequence in Hidden Markov Models via the Viterbi dynamic programming algorithm for speech recognition or part-of-speech tagging.</li>
    <li><strong>Financial Modeling:</strong> Pricing options and optimizing portfolios using dynamic programming approaches like the binomial options pricing model.</li>
    <li><strong>Inventory Management:</strong> Computing optimal reorder points and order quantities in multi-period models via dynamic programming to balance holding and shortage costs.</li>
    <li><strong>Image Processing:</strong> Content-aware image resizing via seam carving, which uses dynamic programming to find and remove low-energy paths through an image.</li>
    <li><strong>Robot Path Planning:</strong> Determining optimal trajectories in grid environments via dynamic programming under motion and obstacle constraints.</li>
  </ul>
</section>

<section id="summary">
  <h2>Summary &amp; Key Takeaways</h2>
  <p>
    Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems, storing the results of those subproblems, and combining them to build up an optimal solution. It relies on two core properties:
  </p>
  <ul>
    <li><strong>Overlapping Subproblems:</strong> Identical subproblems recur and can be solved once and reused.</li>
    <li><strong>Optimal Substructure:</strong> An optimal solution to the overall problem can be constructed from optimal solutions to its subproblems.</li>
  </ul>
  <p>
    You can implement dynamic programming algorithms in two different ways:
    <ul>
    <li><strong>Bottom-Up Tabulation:</strong> Filling a table in dependency order. 
    Best used for problems where every value needs to be computed.</li>
    <li><strong>Top-Down Memoization:</strong> Recursively computing only needed entries and caching results.
    Best used when (hopefully large) portions of the table do not need to be accessed/computed.</li>
    </ul>
    Always define your recurrence clearly, initialize base cases explicitly, and order your computations to match dependencies.
  </p>
  <p>
    By recognizing the dynamic programming pattern&mdash;define subproblem states, derive a recurrence, choose an implementation strategy, and optimize space&mdash;you can transform many exponential-time recursive solutions into efficient polynomial-time solutions.
  </p>
</section>

<section id="resources">
  <h2>Related Links and Resources</h2>
  <ul>
    <li><a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank">Dynamic Programming</a> (Wikipedia) Overview of dynamic programming concepts, history, and key examples.</li>
    <li><a href="https://www.geeksforgeeks.org/dynamic-programming/" target="_blank">Dynamic Programming Tutorial</a> (GeeksforGeeks) Step-by-step guides with code snippets in multiple languages.</li>
    <li><strong>Cormen et al., <em>Introduction to Algorithms</em>, 4th Edition</strong> Chapter 14 is the authoritative textbook treatment on dynamic programming algorithms.</li>
    <li><a href="https://leetcode.com/tag/dynamic-programming/" target="_blank">Practice Problems: Dynamic Programming</a> (LeetCode) Curated set of coding challenges to apply dynamic programming.</li>
    <li><a href="https://www.hackerrank.com/domains/algorithms/dynamic-programming" target="_blank">Dynamic Programming Challenges</a> (HackerRank) Progressive problem sets for honing dynamic programming skills.</li>
    <li><a href="https://medium.com/@beyond_verse/top-10-dynamic-programming-problems-every-programmer-should-solve-4b18ea7eca83">Top 10 Dynamic Programming Problems Every Programmer Should Solve</a> The title speaks for itself.</li>
  </ul>
</section>


<section id="reading-questions">
  <h2>Reading Comprehension Questions</h2>
  <ol>
    <li>What two core properties does dynamic programming rely on?</li>
    <li>In the Coin Row example, what recurrence relation defines <code>M[j]</code>?</li>
    <li>Under what condition is bottom-up tabulation preferred over top-down memoization?</li>
    <li>How can you reduce memory from <code>O(nm)</code> to <code>O(m)</code> according to the implementation tips?</li>
    <li>What is a drawback of top-down memoization on large inputs mentioned in the pitfalls?</li>
    <li>Which limitation describes problems that cannot benefit from dynamic programming?</li>
    <li>What key advantage does dynamic programming offer when a naive recursive solution is exponential-time?</li>
    <li>Which dynamic programming algorithm is cited for sequence alignment in the Real-World Applications?</li>
    <li>Which algorithm is highlighted for network routing in the Real-World Applications?</li>
    <li>When is dynamic programming better than a naive recursive algorithm?</li>
    <!-- New questions about Coin Row and Fibonacci -->
    <li>What base cases are initialized in the memoized Fibonacci algorithm?</li>
    <li>How does memoization prevent repeated work in the Fibonacci computation?</li>
    <li>In the Coin Row algorithm, why do we compare <code>M[j-1]</code> with <code>M[j-2] + C[j]</code>?</li>
    <li>What is the time complexity of the dynamic programming Coin Row algorithm?</li>
    <!-- Problem asking for rolling array algorithm -->
    <li>Give pseudocode for an algorithm that uses the rolling array optimization to compute the 
    \(n\)-th Fibonacci number in \(O(n)\) time and \(O(1)\) space.
    See the following demonstration for inspiration.</li>
  </ol>      
  <div class="embeddedDemoContainer"><iframe class="embeddedDemo" src="/Algorithms/Content/Demos/Dynamic Programming/Fibonacci Improved Demo.html"></iframe>
      </div><br>
  
  <button id="toggleAnswers" class="show-answer" aria-expanded="false">
    Show Answers
  </button>
  <div id="answers" class="answer" hidden>
    <ol>
      <li>
        <strong>Answer:</strong><br>
        It relies on <em>optimal substructure</em> and <em>overlapping subproblems</em>.
      </li>
      <li>
        <strong>Answer:</strong><br>
        \(M[j] = \max\{\,M[j-1],\,M[j-2] + C[j]\}\).
      </li>
      <li>
        <strong>Answer:</strong><br>
        When you need to compute <em>all</em> subproblems, making bottom-up loops clearer and faster.
      </li>
      <li>
        <strong>Answer:</strong><br>
        Use rolling arrays to drop memory from \(O(nm)\) to \(O(m)\).
      </li>
      <li>
        <strong>Answer:</strong><br>
        Deep recursion can cause a stack overflow on large inputs.
      </li>
      <li>
        <strong>Answer:</strong><br>
        Problems without a clear, finite subproblem structure (e.g. continuous or online problems).
      </li>
      <li>
        <strong>Answer:</strong><br>
        It transforms an exponential-time recursive solution into a polynomial-time algorithm by reusing subproblem results.
      </li>
      <li>
        <strong>Answer:</strong><br>
        The Needleman–Wunsch or Smith–Waterman algorithms for sequence alignment.
      </li>
      <li>
        <strong>Answer:</strong><br>
        The Bellman–Ford algorithm used in distance-vector routing.
      </li>
      <li>
        <strong>Answer:</strong><br>
        When the naive recursive solution runs in exponential time but the problem has a polynomial-size state space and overlapping subproblems.
      </li>
      <li>
        <strong>Answer:</strong><br>
        The base cases are initialized as \(fib[0] = 0\) and \(fib[1] = 1\).
      </li>
      <li>
        <strong>Answer:</strong><br>
        Memoization stores each computed \(fib[i]\) so that each value is calculated only once instead of repeatedly.
      </li>
      <li>
        <strong>Answer:</strong><br>
        Because you either skip coin \(j\) (value \(M[j-1]\)) or take it and add its value to the best solution involving coins
        0 through \((j-2)\) (value \(M[j-2] + C[j]\)).
      </li>
      <li>
        <strong>Answer:</strong><br>
        It runs in \(O(n)\) time, filling one table entry per coin in constant time each.
      </li>
      <li>
        <strong>Answer:</strong><br>
        You should have come up with something like the following. Notice that we need three variables
        (in addition to the index variable): one for the current value, one for the previous value, and one for the
        value before that.
        <pre><code>fibonacciRolling(n)
    if(n<2)
        return n
    prev = 0      // represents F[i-2]
    curr = 1      // represents F[i-1]
    for i from 2 to n:
       next = prev + curr
       prev = curr
       curr = next
    return curr  // now holds F[n]</code></pre>
</li>
</ol>
</div>
</section>

<section id="activities">
  <h2>In-Class Activities</h2>
  <ol>
    <li><strong>Whiteboard Sketch:</strong> In pairs, students map out the recursion tree and corresponding dynamic programming table for a simple problem (e.g., Fibonacci) to visualize overlapping subproblems and memoization.</li>
    <li><strong>Algorithm Comparison:</strong> Discuss alternative algorithms to solve the coin-row problem, including their computational complexities,
    and whether or not they are better than the dynamic programming approach.</li>
    <li><strong>Pseudocode Translation:</strong> Given a recurrence relation verbally described by the instructor, students write clear bottom-up pseudocode with proper base-case initialization.</li>
    <li><strong>Error Spotting:</strong> Provide each group with a snippet of buggy dynamic programming code; have them identify and fix common pitfalls like off-by-one loops or missing base cases.</li>
    <li><strong>Space Optimization Challenge:</strong> Students refactor a two-dimensional dynamic programming solution into a rolling-array (one-dimensional) version and calculate the memory savings.</li>
    <li><strong>Real-World Case Study:</strong> In small groups, analyze an application (e.g., sequence alignment) to identify subproblem definitions, recurrence, and table-filling order, then present insights.</li>
    <li><strong>Algorithm Variation:</strong> Pose a twist on a standard dynamic programming problem (e.g., weight limits change dynamically) and have students adapt the recurrence and implementation approach.</li>
    <li><strong>Visualization Exercise:</strong> Use diagramming tools or hand-drawn animations to illustrate how Floyd–Warshall updates shortest-path distances on a tiny graph step by step.</li>
    <li><strong>Reflective Summary:</strong> Individually, students write a one-paragraph summary of when to use dynamic programming and list two pitfalls to watch for, then share and discuss as a class.</li>
  </ol>
</section>


  <!-- Practice Problems -->
  <section id="problems">
    <h2>Homework Problems</h2>
    <h3>Basic</h3>
    <ul>
      <li>Apply the technique to [Simple Problem A].</li>
    </ul>
    <h3>Advanced</h3>
    <ul>
      <li>Combine this technique with [Another Technique] to solve [Complex Problem].</li>
    </ul>
  </section>
  
  </p>
</body>
</html>
