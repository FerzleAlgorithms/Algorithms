<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Technique: Divide-and-Conquer</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>  
  <script src="/Algorithms/scripts/chapterScripts.js"></script>
  <link rel="stylesheet" href="/Algorithms/css/style.css">
  <link rel="stylesheet" href="/Algorithms/css/chapter.css">
</head>
<body>
  <h1>Divide-and-Conquer</h1>
  
  <!-- Motivation & Introduction -->
  <section id="introduction">
  <h2>Introduction</h2>
  <p>
    Divide-and-Conquer is a powerful algorithmic paradigm that solves a problem by breaking it into two or more smaller subproblems of the same type, recursively solving each subproblem, and then combining their solutions to produce an answer to the original problem. This approach underpins many classic algorithms such as Merge Sort and Quicksort, and often yields asymptotically faster solutions than straightforward iterative methods.
  </p>

<h3>Analyzing Divide-and-Conquer Algorithms</h3>
  <p>
    A common way to analyze many divide-and-conquer algorithms is to express their running time with a recurrence relation. Formally, if a problem of size \(n\) can be divided into \(a\) subproblems each of size \(n/b\), and the cost to combine those subsolutions is \(f(n)\), then the overall running time \(T(n)\) satisfies
  </p>
  \[
    T(n) \;=\; a\,T\!\bigl(\tfrac{n}{b}\bigr) \;+\; f(n).
  \]

  <p>
  Recurrences of this form can be solved using the the Master Theorem. 
  Given \(a\), \(b\), and \(f(n)\) (or just a bound \(O(f(n))\), 
  the Master Theorem can be used with very little effort to obtain a 
  closed form for \(T(n)\) most of the time (there are some subtle cases that require a bit more effort).
  Here is one of the most simple versions of the Master Theorem, provided without proof.
  </p>
  <div class="theorem">
  <p>
  <strong>Master Theorem: </strong>
    Consider the recurrence<br>
    \[T(n) = a\,T\bigl(n/b\bigr) + O(n^d),\]
    where \(a \ge 1\), \(b > 1\), and \(d \ge 0\).
  </p>
  <p>The solution depends on the relationship between \(d\) and \(\log_b a\):</p>
  <ul>
    <li><strong>Case 1:</strong> If \(d < \log_b a\), then  
      \(T(n) = \Theta\bigl(n^{\log_b a}\bigr)\).</li>
    <li><strong>Case 2:</strong> If \(d = \log_b a\), then  
      \(T(n) = \Theta\bigl(n^d \,\log n\bigr)\).</li>
    <li><strong>Case 3:</strong> If \(d > \log_b a\), then  
      \(T(n) = \Theta\bigl(n^d\bigr)\).</li>
  </ul>
  <p>
    There are more general variants of the Master Theorem that handle, for example, non-polynomial \(f(n)\), uneven subproblem sizes, or extra logarithmic factors.
  </p>
</div>
    <div class="example-box">
      <strong class="example-title">Example</strong>
      <p>If we can solve a problem by splitting the original problem into two problems of half the size, 
      and combining the solutions to subproblems takes linear time, then  
      \(a = 2\), \(b = 2\), and \(f(n)=O(n)\), so \(d=1\). This yields the recurrence
  \[
    T(n) \;=\; 2\,T\!\bigl(\tfrac{n}{2}\bigr) + O(n).
  \]
   In this case,  
    \(\displaystyle \log_b a = \log_2 2 = 1 = d\), so we fall into Case 2, giving solution
    \(\displaystyle T(n) = \Theta(n\log n).\)
  </p>
  <p>If you recognize this as an analysis of the complexity of Merge Sort, good job!</p>
  </div>

  <p>
  This clean analysis applies neatly to divide-and-conquer algorithms that split their work evenly.
    However, not every divide-and-conquer strategy falls into this simple mold. For instance, 
    Quicksort's running time depends on how the pivot divides the input: in the worst case you get
  </p>
  \[
    T(n) \;=\; T(n-1) + O(n),
  \]
  <p>
    which has closed form \(T(n)=O(n^2)\).  
    Quicksort's average-case is \(O(n\log n)\), but requires a probabilistic analysis that is a lot more involved.
  </p>
</section>

  <section id="examples">
    <h2>Examples</h2>

    <div class="example-box">
      <strong class="example-title">Example 1: Exponentiation (Divide-and-Conquer)</strong>
      <p>
        Suppose you want to compute \(x^n\) for a given real number \(x\) and nonnegative integer \(n\). 
        This is an example of the <a href="?path=Problems/Foundational/Exponentiation">Exponentiation</a> Problem.
        See that page if you are unfamiliar with the concept.
        A straightforward recursive approach might subtract 1 from the exponent at each call, resulting in \(O(n)\) time. Instead, the Divide-and-Conquer version makes two recursive calls on roughly half the exponent, then multiplies the results. Here is one way to implement it, where \(\lfloor n/2\rfloor + \lceil n/2\rceil = n\)
        (Check for yourself that this works for both even and odd values of \(n\)):
      </p>
      <pre><code>function expDC(x, n):
    if n == 0:
      return 1
    // Divide exponent roughly in half
    left  = expDC(x, floor(n / 2))
    right = expDC(x, ceil(n / 2))
    // Conquer by multiplying subresults
    return left * right
</code></pre>
<p>Here is a demonstration that visualizes the recursive calls used in this algorithm.</p>

      <!-- If a demo is available, embed it here; otherwise, remove this container. -->
      <div class="embeddedDemoContainer">
        <iframe
          class="embeddedDemo"
          src="/Algorithms/Content/Demos/Divide-and-Conquer/Exponentiation Demo.html"
          allow="fullscreen"
          name="Exponentiation-DC-demo"
        ></iframe>
      </div>
  <p> 
  If you ran the demo on one or more inputs and looked at the multiplication count, you should see 
  that the algorithm seems to use \(n-1\) multiplications to compute \(x^n\), the same 
  number that the straightforward for loop would use. 
  We will not prove that it is exactly \(n-1\) here (although it indeed is), 
  but we will show that it is linear (\(O(n)\)).
  Let  \(T(n)\) be the number of multiplications performed by <code>expDC(x,n)</code>.  
  On each call, the algorithm does the following:
  <ol>
    <li>Split \(n\) into \(\lceil n/2\rceil\) and \(\lfloor n/2\rfloor\).</li>
    <li>Compute \(y_1 = expDC(x,\lceil n/2\rceil)\) and  
        \(y_2 = expDC(x,\lfloor n/2\rfloor)\).</li>
    <li>Return \(y_1 \times y_2\).</li>
  </ol>
  Note that
  \[
    y_1 \times y_2
    \;=\;
    x^{\lceil n/2\rceil}\times x^{\lfloor n/2\rfloor}
    \;=\;
    x^{\lceil n/2\rceil + \lfloor n/2\rfloor}
    \;=\;
    x^n.
  \]
  </p>
  <p>
    Since combining the two halves requires one multiplication, the recurrence for \(T(n)\) is
  </p>
  \[
    T(n) \;=\; T\!\bigl(\lceil n/2\rceil\bigr)
             \;+\;
             T\!\bigl(\lfloor n/2\rfloor\bigr)
             \;+\;
             1.
  \]
  Because floors and ceiling generally only chnage the solution to a recurrence relation by a constant factor,
  we can simplify this to 
  \[
  \begin{aligned}
    T(n) &= T\bigl(n/2\bigr) + T\bigl(n/2\bigr) + 1,\\
         &= 2\,T\bigl(n/2\bigr) + 1,
  \end{aligned}     
  \]
  which yields \(T(n) = O(n)\) (Check for yourself using the Master Theorem). 
  </p>
  <p>This algorithm is not asymptotically better than a linear loop, 
  and it has higher overhead due to the recursive calls, so it is not actually a great example
  to implement in practice. 
  However, it illustrates the Divide-and-Conquer pattern by splitting the problem into two smaller subproblems
  and combining the solutions.
      </p>
    </div>

    <div class="example-box">
      <strong class="example-title">Example 2: Merge Sort</strong>
      <p>
        Merge Sort is a classic Divide-and-Conquer sorting algorithm. 
        It, of course, solves the the <a href="?path=Problems/Foundational/Sorting">Sorting</a> Problem.
        If you are unfamiliar with this problem, see that page.
        Here is a brief introduction to the idea behind it: Given an array \(A[0..n-1]\), 
        <ol>
          <li>Divide: Split \(A\) into two halves, \(A_{\text{left}}\) and \(A_{\text{right}}\), each of size roughly \(n/2\).</li>
          <li>Conquer: Recursively sort both halves.</li>
          <li>Combine: Merge the two sorted halves into a single sorted array.</li>
        </ol>
      </p>
      <p>
        Below is high-level pseudocode for Merge Sort. 
        For full details, see the 
        <a class="problem" href="?path=Algorithms/Divide-and-Conquer/Merge%20Sort">Merge Sort</a>
        and <a class="problem" href="?path=Algorithms/Greedy/Merge">Merge</a>
        algorithm pages.
      </p>
      
      <pre><code>MergeSort(A):
    if |A| = 1:
        return A
    L = left half of A
    R = right half of A
    MergeSort(L)       // sort first half
    MergeSort(R)       // sort second half
    return Merge(L, R) // combine halves

Merge(L, R):  // Merge two already-sorted lists L and R
    result = empty list
    while L and R are not empty:
        if L[0] <= R[0]:
            append L[0] to result
            remove L[0] from L
        else:
            append R[0] to result
            remove R[0] from R
    // one list is empty. just append the rest of the other list
    append all remaining elements of L (if any) to result
    append all remaining elements of R (if any) to result
    return result
</code></pre>

<p>Here is a demonstration that takes you step-by-step through the algorithm.</p>
 <div class="embeddedDemoContainer">
        <iframe
          class="embeddedDemo"
          src="/Algorithms/Content/Demos/Divide-and-Conquer/Merge Sort Demo.html"
          allow="fullscreen"
          name="MergeSort-DC-demo"
          style="height: 400px; min-height: 400px;"
        ></iframe>
      </div>
    
      <p>Computing the time taken by Merge Sort is pretty straightforward.  
        Splitting the array takes \(O(1)\) (just computing the midpoint). The two recursive calls each sort a subarray of size \(\frac{n}{2}\). Merging two sorted arrays of total size \(n\) takes \(O(n)\). Hence, the recurrence is
        \[
          T(n) \;=\; 2\,T\!\bigl(\tfrac{n}{2}\bigr) \;+\; O(n),
        \]
        which solves to \(T(n) = O(n \log n)\) (We talked about this earlier&mdash;just use the Master Theorem).
         Because Merge Sort splits evenly and merges in linear time, 
         it is asymptotically optimal among comparison-based sorts.
         Merge Sort is a great example of a successful application of the divide-and-conquer technique.
      </p>
     
    </div>
  </section>
  
  
<section id="algorithms-using-technique">
  <h2>Algorithms Using This Technique</h2>
  <ul>
    <li>
      <a class="problem" href="?path=Algorithms/Divide-and-Conquer/Quicksort">Quicksort</a>: selects a pivot, 
      puts it in its correct place, 
      the elements less than the pivot to the left and 
      the elements greater than pivot to the right, then recursively sorts each part; 
      average-case runtime \(O(n \log n)\), worst-case \(O(n^2)\).
    </li>
    <li>
      <a class="problem" href="?path=Algorithms/Divide-and-Conquer/Merge Sort">Merge Sort</a>: divides the array in half, recursively sorts each half, and merges them in \(O(n)\) time, for an overall runtime of \(O(n \log n)\).
    </li>
    <li>
      <a class="problem" href="?path=Algorithms/Divide-and-Conquer/ClosestPair">Closest Pair of Points</a>: splits points by x-coordinate, recursively finds the closest pair in each half, and checks cross-boundary pairs in linear time, yielding \(O(n \log n)\).
    </li>
    <li>
      <a class="problem" href="?path=Algorithms/Divide-and-Conquer/Strassens">Strassen's Matrix Multiplication</a>: multiplies two \(n\times n\) matrices by computing seven products of size \(n/2\), achieving runtime \(O(n^{\log_2 7})\approx O(n^{2.81})\).
    </li>
    <li>
      <a class="problem" href="?path=Algorithms/Divide-and-Conquer/Exponentiation">Exponentiation</a>: 
      computes \(x^n\) by recursively computing \(x^{\lfloor n/2\rfloor}\) and \(x^{\lceil n/2\rceil}\)
      and multiplying them. Requires \(n-1=O(n)\) multiplications. 
      This is a bad example of the use of divide-and-conquer since a simple for loop does the same number
      of multiplications, uses less space, and is easier to implement.
    </li>
    <li>
      <a class="problem" href="?path=Algorithms/Divide-and-Conquer/Convex Hull">Convex Hull</a>: splits the point set, computes hulls in each subset, and merges the hulls in linear time, for \(O(n \log n)\) overall.
    </li>
    <li>
      <strong>Karatsuba Integer Multiplication</strong>: splits integers into halves, 
      uses three half-size multiplications instead of four, and takes linear time to combine subsolutions, 
      yielding runtime \(O(n^{\log_2 3})\approx O(n^{1.59})\).
    </li>
    <li>
      <strong>Fast Fourier Transform (FFT)</strong>: Computes the Discrete Fourier Transform (DFT)
      by recursively separating even and odd coefficients, achieving \(O(n \log n)\).
    </li>
  </ul>
</section>

<section id="applicability">
  <h2>When to Use</h2>
  The following are some of the reasons that algorithms like Quicksort, Merge Sort, Strassen's Algorithm, 
  and divide-and-conquer algorithms for problems like Convex Hull, Closest Pair, Discrete Fourier Transforms,
  and many other problems work well. 
  <ul>
    <li>
      <strong>Natural Divide Step:</strong>
      When the problem can be split into two or more independent subproblems of smaller size 
      (often roughly size \(n/b\) for some \(b>1\)).
    </li>
    <li>
      <strong>Efficient Combine Step:</strong>
      When merging or combining \(a\) subsolutions of total size \(n\) takes \(O(n^d)\) with \(d \le \log_b a\), the recurrence
      \[
        T(n) = a\,T(n/b) + O(n^d)
      \]
      falls under case 1 (\(d < \log_b a\)) or case 2 (\(d = \log_b a\)) of the Master Theorem, 
      both of which improve on algorithms whose best non-recursive versions cost \(O(n^d)\). Examples:
      <ul>
        <li>
          <em>Merge Sort:</em>
          \(a=2\), \(b=2\), \(d=1=\log_2 2\).  
          Case 2 gives  
          \(T(n)=2\,T(n/2)+O(n)=O(n\log n)\),  
          improving over simple quadratic sorts (e.g. Insertion Sort or Bubble Sort) that run in \(O(n^2)\).
        </li>
        <li>
          <em>Strassen's Matrix Multiply:</em>
          \(a=7\), \(b=2\), \(d=2<\log_2 7\approx2.81\).  
          Case 1 yields  
          \(T(n)=7\,T(n/2)+O(n^2)=O(n^{\log_2 7})\approx O(n^{2.81})\),  
          improving on the classical triple-loop algorithm's \(O(n^3)\) cost.
        </li>
      </ul>
    </li>
    <li>
      <strong>Balanced Subproblems:</strong>
      When each recursive call works on a piece of similar size, so you avoid "skinny" partitions that degrade
       performance.
    </li>
    <li>
      <strong>Parallelism:</strong>
      When independent subproblems can run concurrently on multiple cores or machines (for example, a parallel Merge Sort, Quicksort, or Strassen's matrix multiplication).
    </li>
    <li>
      <strong>Memory &amp; Cache Efficiency:</strong>
      When breaking the problem into smaller pieces helps each subproblem fit in fast memory (cache/registers).
    </li>
    <li>
      <strong>Large Input Sizes:</strong>
      When \(n\) is large enough that the overhead of recursion and function calls is small relative to the asymptotic gains.
    </li>
  </ul>
  <p>
    In short, reach for divide-and-conquer whenever a problem naturally breaks into smaller 
    subproblems that can solved independetly and can be comibed efficiently (often in linear time) 
    such that the cost of solving and combining them recursively yields clear asymptotic or practical benefits.
  </p>
</section>

<section id="limitations">
  <h2>Limitations</h2>
  <ul>
    <li>
      <strong>Recursive Overhead:</strong>
      Function-call overhead and extra stack frames can dominate when \(n\) is small or when the combine step is simple, making an iterative or non-recursive method faster in practice.
    </li>
    <li>
      <strong>Inefficient Combine Steps:</strong>
      If combining subresults takes superlinear time \(O(n^d)\) with \(d > \log_b a\), then
      \[
        T(n) = a\,T(n/b) + O(n^d) = O(n^d),
      \]
      offering no asymptotic advantage over direct algorithms.
    </li>
    <li>
      <strong>Unbalanced Splits:</strong>
      When subproblem sizes are highly skewed, the result can be much less efficient than with even splits,
      sometimes negating all of the benefit of the approach.
      For instance, when Quicksort has poor pivot choices, the algorithm degrades to \(O(n^2)\)
      instead of the average-case \(O(n\log n)\).
      In the case of Quicksort, there are ways of avoiding these bad splits, but when that is not possible,
      divide-and-conquer may not be a good choice.
    </li>
    <li>
      <strong>Stack and Tail-Call Limits:</strong>
      Deep recursion may exhaust the call stack. In languages without tail-call optimization, you may need to convert to an explicit stack or iterative loop.
    </li>
    <li>
      <strong>Overlapping Subproblems:</strong>
      Divide-and-conquer repeats work on overlapping subproblems. When overlap is significant, dynamic programming or memoization often yields better performance.
    </li>
    <li>
      <strong>Extra Memory Usage:</strong>
      Many divide-and-conquer algorithms allocate temporary storage (for example, Merge Sort requires \(O(n)\) extra space), which can be a drawback in memory-constrained environments.
    </li>
    <li>
      <strong>Parallelization Overhead:</strong>
      While subproblems are parallelizable, task-creation, synchronization, and limited memory bandwidth can reduce or negate expected speedups.
    </li>
    <li>
      <strong>Constant Factors in Practice:</strong>
      The improved asymptotic bound may hide large constant factors. 
      Thus, it is often the case that simpler algorithms are better for smaller inputs.
      For instance, Insertion Sort is often better than Quicksort for small arrays (e.g. \(n=10\) or \(20\)),
      and Strassen's Algorithm is only better than the standard \(O(n^3)\) algorithm when  
      \(n>1000\), and even then the gains can be quite moderate (some estimates are about 10% improvement).
    </li>
  </ul>
</section>
<section id="implementation-tips">
  <h2>Implementation Tips</h2>
  <ul>
    <li>
      <strong>Choose a good base case threshold:</strong>
      For small \(n\), switch to an iterative algorithm to avoid recursion overhead. The value of \(n\)
      and what algorithm to switch to depends heavily on each problem. For instance with Quicksort, 
      Insertion Sort is often called when \(n<20\).
    </li>
    <li>
      <strong>Balanced splitting:</strong>
      Ensure subproblems are roughly equal in size (for example, use median-of-three or random pivot selection in Quicksort) for optimal performance.
    </li>
    <li>
      <strong>Avoid repeated allocations:</strong>
      Preallocate temporary buffers or auxiliary arrays once and pass them through recursive calls rather than reallocating at each level.
    </li>
    <li>
      <strong>Optimize the combine step:</strong>
      Implement merges or other combine operations efficiently. 
      For instance, with some effort and the use of an auxilliary array, the Merge step of Merge Sort can be
      made a lot more efficient by merging back and forth from the original and auxiliary arrays. Of course 
      this makes the algorithm more complicated to implement correctly, but it does make it faster.
    </li>
    <li>
      <strong>Use index arithmetic instead of slicing:</strong>
      Pass index ranges (\(\mathrm{lo},\mathrm{hi}\)) or pointers rather than copying subarrays to reduce both time and space overhead.
    </li>
    <li>
      <strong>Prevent integer overflow in midpoint calculation:</strong>
      Compute 
      \[
        \mathrm{mid} = \mathrm{lo} + \frac{\mathrm{hi}-\mathrm{lo}}{2}
      \]
      instead of \((\mathrm{lo}+\mathrm{hi})/2\). This is mainly relevant if you are working with very large arrays.
    </li>
    <li>
      <strong>Watch for off-by-one errors in base cases:</strong>
      Handle very small inputs (for example, \(n \le 1\)) explicitly and decide consistently how to split odd sizes.
    </li>
    <li>
      <strong>Tail-call optimization / iterative conversion:</strong>
      Recurse on the smaller subproblem first and use loops or an explicit stack for the larger one to limit recursion depth.
    </li>
    <li>
      <strong>In-place vs out-of-place combine:</strong>
      Decide whether to overwrite the input or use auxiliary space&mdash;an in-place algorithm saves memory but can be trickier to implement, and sometimes sacrifices time efficiency.
    </li>
    <li>
      <strong>Memoize overlapping subproblems:</strong>
      Add memoization when subproblems repeat (for example, naive Fibonacci) to avoid exponential recomputation.
    </li>
    <li>
      <strong>Parallel divide-and-conquer granularity:</strong>
      Only spawn concurrent tasks for sufficiently large subproblems to avoid thread-creation and synchronization overhead.
    </li>
  </ul>
</section>

<section id="common-pitfalls">
  <h2>Common Pitfalls</h2>
  <ul>
    <li>
      <strong>Off-by-one errors:</strong>
      Incorrect bounds (for example, using \(\lfloor n/2\rfloor\) vs \(\lceil n/2\rceil\)) can omit or duplicate elements.
    </li>
    <li>
      <strong>Stack overflow:</strong>
      Deep or unbalanced recursion may exceed the call stack; guard against this with tail recursion or an explicit stack.
    </li>
    <li>
      <strong>Excessive memory usage:</strong>
      Copying subarrays at each recursion level can lead to \(O(n\log n)\) space instead of \(O(n)\).
    </li>
    <li>
      <strong>Base-case overhead:</strong>
      Forgetting to switch to a simpler method for small \(n\) may make the recursive version slower than an iterative algorithm.
    </li>
    <li>
      <strong>Incorrect combine complexity:</strong>
      Implementing merge or multiply steps suboptimally can raise \(d\) above \(\log_b a\), losing asymptotic improvements.
    </li>
    <li>
      <strong>Neglecting non-power-of-two sizes:</strong>
      Assuming \(n\) is a power of two can lead to incorrect behavior or uneven splits.
    </li>
    <li>
      <strong>Recomputing overlapping subproblems:</strong>
      Without memoization, overlapping work (for example, naive divide-and-conquer Fibonacci) can blow up exponentially.
    </li>
    <li>
      <strong>Large constant factors:</strong>
      High constants in recursion or combine steps may make an asymptotically better algorithm slower for practical \(n\).
    </li>
    <li>
      <strong>Unbalanced splits:</strong>
      Poor pivot choices or uneven division can degrade performance significantly.
    </li>
    <li>
      <strong>Parallelization overhead:</strong>
      Task creation, synchronization, and memory-bandwidth limits can negate expected speedups.
    </li>
  </ul>
</section>


<section id="real-world-applications">
  <h2>Real-World Applications</h2>
  <ul>
    <li>
      <strong>Standard Library Sorting:</strong>
      Languages like Java (<code>Arrays.sort()</code>) and C++ (<code>std::sort</code>) employ Quicksort, Merge Sort, or hybrids to sort large arrays in \(O(n \log n)\).
    </li>
    <li>
      <strong>Signal Processing:</strong>
      The Fast Fourier Transform (FFT) computes the \(n\)-point discrete Fourier transform in \(O(n \log n)\) by recursively splitting data into even and odd components.
    </li>
    <li>
      <strong>Computational Geometry:</strong>
      Algorithms for Closest Pair of Points and Convex Hull split point sets and merge partial hulls or nearest pairs in \(O(n \log n)\).
    </li>
    <li>
      <strong>External Sorting:</strong>
      External Merge Sort handles data larger than memory by creating sorted runs on disk and merging them in \(O(n \log n)\) I/O passes.
    </li>
    <li>
      <strong>Cryptographic Multiplication:</strong>
      Karatsuba's and Toom-Cook algorithms multiply large integers or polynomials in subquadratic time 
      (e.g. \(O(n^{\log_{2}3})\approx O(n^{1.59})\)), critical for RSA and ECC (Elliptic Curve Cryptography).
    </li>
    <li>
      <strong>Image Compression & Representation:</strong>
      Quadtree decomposition encodes images by recursively subdividing regions until pixel values are uniform, enabling efficient storage and level-of-detail rendering.
    </li>
    <li>
      <strong>Big Data & Parallel Processing:</strong>
      MapReduce frameworks and parallel Quicksort distribute partitions across compute nodes, sorting massive datasets concurrently.
    </li>
    <li>
      <strong>Financial Simulations:</strong>
      Monte Carlo methods for option pricing split simulation trials into independent batches and combine results to estimate risk and fair values more efficiently.
    </li>
    <li>
      <strong>Spatial & Interval Data Structures:</strong>
      Building segment trees or kd-trees in \(O(n \log n)\) uses divide-and-conquer to support fast range and nearest-neighbor queries.
    </li>
  </ul>
</section>

<section id="summary">
  <h2>Summary &amp; Key Takeaways</h2>
  <p>
    Divide-and-conquer works by <em>dividing</em> a problem of size \(n\) into \(a\) independent subproblems of size \(n/b\), <em>conquering</em> each by recursion, and <em>combining</em> their results in \(O(f(n))\) time.  Writing \(f(n)=O(n^d)\) for some constant \(d\ge0\) gives the recurrence
    \[
      T(n) \;=\; a\,T\!\bigl(\tfrac{n}{b}\bigr)\;+\;O(n^d).
    \]
    The Master Theorem then compares \(d\) to \(\log_b a\):
  </p>
  <ul>
    <li>
      <strong>If \(d < \log_b a\):</strong>  
      the leaf-level work \(n^{\log_b a}\) dominates, so  
      \(T(n)=O\bigl(n^{\log_b a}\bigr)\).
    </li>
    <li>
      <strong>If \(d = \log_b a\):</strong>  
      divide and combine costs balance, so  
      \(T(n)=O\bigl(n^d\log n\bigr)\).
    </li>
    <li>
      <strong>If \(d > \log_b a\):</strong>  
      the combine cost dominates, so  
      \(T(n)=O\bigl(n^d\bigr)\).
    </li>
  </ul>
  <p>
    <strong>Key steps:</strong>
  </p>
  <ul>
    <li><strong>Divide:</strong> split into roughly equal subproblems.</li>
    <li><strong>Conquer:</strong> recurse until a simple base case.</li>
    <li><strong>Combine:</strong> merge solutions in \(O(n^d)\) time.</li>
  </ul>
  <p>
    <strong>Why it matters:</strong> When \(d\le\log_b a\), divide-and-conquer yields asymptotic speedups 
    (e.g. \(O(n\log n)\) or better versus \(O(n^2)\)), plus good cache locality and easy parallelism. 
  </p><p> 
    <strong>Watch out for:</strong> Recursion overhead, extra memory use, 
    and unbalanced splits that push you into the third case.
  </p>
</section>

<h2>Related Links and Resources</h2>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm" target="_blank">Divide-and-conquer algorithm (Wikipedia)</a> Overview of the paradigm, examples such as Merge Sort and Quicksort, and discussion of the recurrence \(T(n)=a\,T(n/b)+f(n)\).</li>
  <li><a href="https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)" target="_blank">Master theorem (analysis of algorithms) (Wikipedia)</a> Precise statement of the Master Theorem cases for solving recurrences of the form \(T(n)=a\,T(n/b)+O(n^d)\).</li>
  <li><a href="https://www.geeksforgeeks.org/dsa/divide-and-conquer/" target="_blank">Divide and Conquer Algorithm (GeeksforGeeks)</a> Step-by-step guide with pseudocode for key examples like Merge Sort and  Quicksort.</li>
  <li><a href="https://www.programiz.com/dsa/divide-and-conquer" target="_blank">Divide and Conquer Algorithm (Programiz)</a> Clear explanation of the divide, conquer, and combine steps, illustrated with Merge Sort in Java, Python, and C++.</li>
  <li><a href="https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/resources/lecture-2-divide-conquer-convex-hull-median-finding/" target="_blank">Lecture 2: Divide &amp; Conquer: Convex Hull, Median Finding (MIT OCW)</a> PDF notes and video on applying D&C to geometric and selection problems.</li>
  <li><a href="https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/resources/lecture-3-divide-conquer-fft/" target="_blank">Lecture 3: Divide &amp; Conquer: FFT (MIT OCW)</a> Lecture notes and video covering FFT as a D&C algorithm for polynomial evaluation.</li>
  <li><a href="https://visualgo.net/en/sorting" target="_blank">VisuAlgo: Merge Sort &amp; Quick Sort Visualizations</a> Interactive animations demonstrating the divide-and-conquer structure of Merge Sort and Quicksort.</li>
</ul>


<section id="reading-questions">
  <h2>Reading Comprehension Questions</h2>
  <ol>
    <li>
      <strong>Divide-Conquer Pattern</strong><br>
      Describe the three main steps of a divide-and-conquer algorithm.
    </li>
    <li>
      <strong>Recurrence Derivation</strong><br>
      How does dividing into \(a\) subproblems of size \(n/b\) with combine cost \(O(n^d)\) lead to the recurrence  
      \[
        T(n) = a\,T\bigl(n/b\bigr) + O(n^d)\;?
      \]
    </li>
    <li>
      <strong>Master Theorem Cases</strong><br>
      What are the three cases of the Master Theorem based on comparing \(d\) and \(\log_b a\), and what is the bound on \(T(n)\) in each case?
    </li>
    <li>
      <strong>Master Theorem Examples</strong><br>
      For Merge Sort, Strassen's Matrix Multiplication, and Karatsuba Multiplication, identify \(a\), \(b\), and \(d\), and state which Master Theorem case applies.
    </li>
    <li>
      <strong>Exponentiation Example</strong><br>
      In the exponentiation example, why does splitting \(n\) into \(\lfloor n/2\rfloor\) and \(\lceil n/2\rceil\) still compute \(x^n\) correctly? Why is it not a good application of divide-and-conquer?
    </li>
    <li>
      <strong>Merge Sort Space</strong><br>
      Why does Merge Sort require \(O(n)\) extra space, and what role does the auxiliary array play?
    </li>
    <li>
      <strong>Merge Complexity</strong><br>
      Explain why merging two sorted lists of total length \(n\) takes \(O(n)\) time, and how this influences Merge Sort's overall runtime.
    </li>
    <li>
      <strong>Limitations</strong><br>
      Name one limitation of divide-and-conquer algorithms discussed on the page and explain its impact.
    </li>
    <li>
      <strong>Implementation Tip</strong><br>
      Choose one implementation tip from the page and describe how it improves performance or correctness.
    </li>
    <li>
      <strong>Common Pitfall</strong><br>
      Name one common pitfall and explain how it can degrade performance or cause errors.
    </li>
    <li>
      <strong>Parallelism Benefit</strong><br>
      Describe one scenario where divide-and-conquer's natural parallelism yields real-world speedups.
    </li>
    <li>
      <strong>When to Use</strong><br>
      Summarize one key condition under which divide-and-conquer is especially effective.
    </li>
  </ol>
  <button id="toggleAnswers" aria-expanded="false">Show Answers</button>
  <div id="answers" hidden>
    <ol>
      <li>
        <strong>Answer:</strong><br>
        <em>Divide:</em> split the input of size \(n\) into \(a\) subproblems of size \(n/b\).<br>
        <em>Conquer:</em> solve each subproblem recursively until a base case is reached.<br>
        <em>Combine:</em> merge or assemble subresults in \(O(n^d)\) time to form the final solution.
      </li>
      <li>
        <strong>Answer:</strong><br>
        You perform \(a\) recursive calls on inputs of size \(n/b\), each costing \(T(n/b)\), and then do \(O(n^d)\) work to combine their results, yielding  
        \[
          T(n)=a\,T(n/b)+O(n^d).
        \]
      </li>
      <li>
        <strong>Answer:</strong><br>
        Compare \(d\) to \(\log_b a\):<ul><li>
        If \(d<\log_b a\), then \(T(n)=O(n^{\log_b a})\).
        </li><li>If \(d=\log_b a\), then \(T(n)=O(n^d\log n)\).
        </li><li>If \(d>\log_b a\), then \(T(n)=O(n^d)\).
        </li></ul>
      </li>
      <li>
        <strong>Answer:</strong><br>
        <ul><li>Merge Sort: \(a=2\), \(b=2\), \(d=1\); since \(d=\log_2 2\), case 2 applies, yielding \(O(n\log n)\).
        </li><li>Strassen's: \(a=7\), \(b=2\), \(d=2\); since \(d<\log_2 7\), case 1 applies yielding \(O(n^{\log_2 7})\).
        </li><li>Karatsuba: \(a=3\), \(b=2\), \(d=1\); since \(d<\log_2 3\), case 1 applies yielding \(O(n^{\log_2 3})\).
        </li></ul>
      </li>
      <li>
        <strong>Answer:</strong><br>
        Splitting into \(\lfloor n/2\rfloor\) and \(\lceil n/2\rceil\) works because  
        \[
          \lfloor n/2\rfloor + \lceil n/2\rceil = n,
        \]
        so the recursive calls compute \(x^{\lfloor n/2\rfloor}\) and \(x^{\lceil n/2\rceil}\), and multiplying them gives  
        \(\,x^{\lfloor n/2\rfloor}\times x^{\lceil n/2\rceil}=x^n\).  
        It is not a good divide-and-conquer example because it uses \(n-1\) multiplications&mdash;the same as a simple loop&mdash;and adds recursion overhead without reducing the asymptotic work.
      </li>
      <li>
        <strong>Answer:</strong><br>
        Merge Sort needs an auxiliary array of size \(n\) to hold merged output when combining two sorted halves. This buffer lets you merge in \(O(n)\) time before copying back.
      </li>
      <li>
        <strong>Answer:</strong><br>
        Merging two sorted lists of total length \(n\) takes \(O(n)\) because each element is examined and placed exactly once. Since Merge Sort does this at each of \(\log n\) levels, the total is \(O(n\log n)\).
      </li>
      <li>
        <strong>Answer:</strong><br>
        One limitation is extra memory usage: some algorithms (e.g. Merge Sort) require \(O(n)\) auxiliary space, which can be prohibitive in memory-constrained environments.
      </li>
      <li>
        <strong>Answer:</strong><br>
        Preallocate and reuse temporary buffers across recursive calls to avoid repeated allocations, reducing both time and memory overhead.
      </li>
      <li>
        <strong>Answer:</strong><br>
        Off-by-one errors in computing split bounds can omit or duplicate elements, leading to incorrect results or infinite recursion.
      </li>
      <li>
        <strong>Answer:</strong><br>
        In parallel Merge Sort, the two halves can be sorted simultaneously on separate cores, cutting wall-clock time roughly in half on a two-core machine for large inputs (and even more given more cores).
      </li>
      <li>
        <strong>Answer:</strong><br>
        Divide-and-conquer is effective when subproblems are independent and the combine cost \(O(n^d)\) does not exceed the leaf work \(O(n^{\log_b a})\), ensuring asymptotic improvement and clear parallelism.
      </li>
    </ol>
  </div>
</section>

<section id="in-class-activities">
  <h2>In-Class Activities</h2>
  <ol>
    <li>
      <strong>Recurrence Identification and Solution:</strong>  
      Look at the algorithms from the page (e.g. Strassen's, Merge Sort, etc.). 
      For each, state recurrence \(T(n)=a\,T(n/b)+O(n^d)\), identify the constants \(a\), \(b\) and \(d\),
      and use the Master Theorem to determine computational complexity.
    </li>
    <li>
      <strong>Master Theorem Practice:</strong>  
      For each of the following recurrences, apply the Master Theorem and give the tight bound on \(T(n)\):  
      <ol type='a'>
        <li>\(T(n)=2\,T(n/2)+O(1)\)</li>
        <li>\(T(n)=3\,T(n/3)+O(n)\)</li>
        <li>\(T(n)=4\,T(n/2)+O(n^2)\)</li>
      </ol>
    </li>
    <li>
      <strong>Merge Sort Trace:</strong>  
      Hand-execute Merge Sort on the array \([7,2,5,3,8,1,4,6]\). Draw the recursive calls using a tree-like
      structure.
      Show each divide step, and for one merge step, count how many comparisons and moves occur.
    </li>
    <li>
      <strong>Recursive vs Iterative Exponentiation:</strong>  
      For \(n=13\), simulate both the divide-and-conquer exponentiation and the simple loop. 
      Count total multiplications and discuss the overhead of recursion.
    </li>
    <li>
      <strong>Recursion Tree Drawing:</strong>  
      Draw the recursion tree for \(T(n)=2\,T(n/2)+O(n)\). Label work at each level and sum to confirm \(O(n\log n)\).
    </li>
    <li>
      <strong>The Good and Bads of Mergesort:</strong>  
      Think about the various downsides of Merge Sort and ways you can deal with them.
      Then discuss the upsides of Merge Sort. Compare it to other sorting algorithms.
    </li>
    <li><strong>Better Exponentiation:</strong>
    It turns out that the bad divide-and-conquer exponentiation algorithm can be redeemed without that
    much work. Discuss how the algorithm can be fixed to actually be worth implementing.
    What design technique does your algorithm use?
    </li>
    <li>
      <strong>Counterfeit Coin Search:</strong><br>
      You have 27 coins, one heavier than the rest, and a two-pan balance scale
      (that allows you to tell which of two piles of coins is heavier). 
      <ol type='a'>
        <li>Devise a divide-and-conquer strategy to solve this problem. 
        Your algorithm should be as efficient as possible,
      where efficiency is measured by the number of wieghings.</li>
        <li>Derive a recurrence relation for your algorithm and solve it.</li>
        <li>Generalize your solution to \(n\) coins (assume \(n\) is a power of 3).</li>
      </ol>
    </li>
    <li>
      <strong>Count Inversions</strong><br>
      An inversion in an array \(A\) is a pair \((i,j)\) such that \(i \lt j\) and \(A[i] \gt A[j]\). 
      In other words, the elements are "out of order" compared to being in a sorted array.
      Given an array of \(n\) numbers, our goal is to count the number of inversions.
      <ol type='a'>
        <li>Write pseudocode for a simple iterative algorithm to solve the problem.</li>
        <li>Determine the computational complexity of your algorithm</li>
        <li>Write pseudocode for a divide-and-conquer algorithm that splits the array in half, 
        recursively counts inversions in each half, and counts "cross-inversions" 
        during a merge in \(O(n)\) time.</li>
        <li>Derive and solve a recurrence relation for your algorithm.</li>
        <li>Compare the two algorithms. Which approach is better and why?</li>
      </ol>
    </li>
  </ol>
</section>
<section id="problems">
  <h2>Homework Problems</h2>

  <section id="basic-problems">
    <h3>Basic Problems</h3>
    <ol>
      <li>
        <strong>Master Theorem Practice:</strong><br>
        Use the Master Theorem to find tight bounds for:
        <ol type='a'>
          <li>\(T(n) = 3\,T(n/3) + O(n)\)</li>
          <li>\(T(n) = 4\,T(n/2) + O(n^2)\)</li>
          <li>\(T(n) = 7\,T(n/3) + O(n^2)\)</li>
          <li>\(T(n) = 4\,T(n/5) + O(n^3)\)</li>
          <li>\(T(n) = 7\,T(n/8) + O(n^2)\)</li>
        </ol>
      </li>
     
      <li>
        <strong>Min and Max:</strong><br>
        Design a divide-and-conquer algorithm to find both the minimum and maximum of an 
        array of \(n\) numbers using fewer than \(2n-2\) comparisons. Derive and solve its recurrence.
        Is your algorithm better than a brute-force or greedy approach? Explain.
      </li>
      <li>
        <strong>Counterfeit Coin:</strong><br>
        You have \(3^n\) coins with one heavier counterfeit and a balance scale. 
        Devise a Divide-and-Conquer strategy splitting into three piles to solve this problem.
        Provide clear code/pseudocode, derive and solve a recurrence relation for the worst-case complexity
        o your algorithm. 
      </li>
      <li>
        <strong>Inversion Counting:</strong><br> 
        An inversion in an array \(A\) is a pair \((i,j)\) such that \(i \lt j\) and \(A[i] \gt A[j]\). 
        In other words, the elements are "out of order" compared to being in a sorted array.
        Describe a divide-and-conquer algorithm to count inversions in an array with complexity \(O(n\log n)\) time. 
        Provide pseudocode, derive and solve the recurrence for its complexity, 
        and compare it to the naive \(O(n^2)\) method.
      </li>
      <li>
  <strong>Majority Element Variants</strong><br>
  The <em>majority element</em> of an array \(A\) is an element that appears 
  more than \(\lfloor n/2\rfloor\) times.  
  This problem explores various algorithms to find the majority element of an array.
  For the purposes of this problem, we will assume that the array <i>has</i> a majority element,
  and we just need to determine what its value is.
  <ol type="a">
    <li>
      <strong>Divide-and-Conquer</strong><br>
      Consider this pseudocode:<br>
      <pre><code>majorityDC(A, lo, hi):
    if lo == hi:
      return A[lo]
    mid = floor((lo + hi)/2)
    leftMaj  = majorityDC(A, lo, mid)
    rightMaj = majorityDC(A, mid+1, hi)
    if leftMaj == rightMaj:
      return leftMaj
    leftCount  = count(A, lo, hi, leftMaj)    // O(hi-lo+1)
    rightCount = count(A, lo, hi, rightMaj)   // O(hi-lo+1)
    return (leftCount > rightCount) ? leftMaj : rightMaj
</code></pre>
      <ol type='i'>
        <li>Give and algorithm for and determine the computational complexity of an implementation of 
        <code>count(A, lo, hi, value)</code> that counts the number of occurrences of \(value\)
        in the array \(A\) between indices \(lo\) and \(hi\) (inclusive of both).</li>
        <li>Derive the recurrence for the running time of <code>majorityDC(A, 0, n-1)</code> and solve it.</li>
      </ol>
    </li>
    <li>
      <strong>Hash-Map Counting</strong><br>
      Describe an algorithm that scans the array once, uses a hash map to track frequencies, and then finds any key whose count exceeds \(\frac{hi - lo + 1}{2}\).  Give its time and space complexity.
    </li>
    <li>
      <strong>Boyer-Moore Majority Vote</strong><br>
      Look up the Boyer-Moore voting algorithm. Write it out in pseudocode, 
      prove that it finds the majority element in one pass, and state its time and space complexity.
    </li>
    <li>
      <strong>Comparison</strong><br>
      Compare the three methods in terms of:
      <ul>
        <li>Time complexity</li>
        <li>Space usage</li>
        <li>Practical considerations (e.g. ease of implementation, constant factors, stability under adversarial inputs)</li>
      </ul>
    </li>
  </ol>
</li>
    </ol>
  </section>
  
<section id="advanced-problems">
  <h2>Advanced Problems</h2>
  <ol>
    <li>
      <strong>Maximum Subarray Sum</strong><br>
      Given an array <code>A[0..n-1]</code> of possibly positive and negative numbers, the goal is to find the contiguous subarray <code>A[i..j]</code> whose sum is maximal.
      <ol type="a">
        <li>Devise a brute-force algorithm to solve the problem.</li>
        <li>Determine the worst-case time complexity of your brute-force method.</li>
        <li>Devise a divide-and-conquer algorithm: clearly describe the divide, conquer, and combine steps.</li>
        <li>Give and solve a recurrence for its worst-case running time.</li>
        <li>(Optional) Devise a linear-time algorithm 
        (Hint: If you get stuck an your professor allows it, research Kadane's method).</li>
        <li>Determine the worst-case complexity of your algorithm from part (e).</li>
        <li>Compare and contrast the algorithms from parts (a), (c), and (e).</li>
      </ol>
    </li>
    <li>
      <strong>Skyline Problem</strong><br>  
      You are given a list of buildings, each described by a 
      triple \((L_i,H_i,R_i)\) meaning a rectangle spanning from x-coordinate \(L_i\) 
      to \(R_i\) with height \(H_i\).  The desired output is the skyline: 
      a sequence of key points \(\{(x_0,h_0),(x_1,h_1),\dots\}\) where the visible height changes,
      given in increasing order of the \(x\)-coordinate.
      <blockquote>
      <strong>Example:</strong><br>
      <strong>Input:</strong> five buildings \((1,2,3)\), \((2,4,5)\), \((4,3,6)\), \((7,5,9)\), \((8,2,10)\).<br>
      <strong>Output:</strong> key points \((1,2),(2,4),(5,3),(6,0),(7,5),(9,2),(10,0)\).  
    
      <p>Here is a sketch of the input (colored boxes) and output (black lines). 
      Notice that the output points are not always the high points, but instead the new height of the next segment,
      which might be larger or smaller than the curent height.</p>
      <svg width="400" height="280" viewBox="-1 0.5 12 6" preserveAspectRatio="none">
        <!-- buildings -->
        <rect x="2" y="2" width="3" height="4" fill="lightgreen"/>
        <rect x="4" y="3" width="2" height="3" fill="lightblue"/>
        <rect x="7" y="1" width="2" height="5" fill="lightgoldenrodyellow"/>
        <rect x="8" y="4" width="2" height="2" fill="lightpink"/>
        <rect x="1" y="4" width="2" height="2" fill="lightcoral"/>
    
        <!-- corner labels -->
        <text x="0.5" y="3.8" font-size="0.3">(1,2)</text>
        <text x="2.5" y="3.8" font-size="0.3">(3,2)</text>
        <text x="1.5" y="1.8" font-size="0.3">(2,4)</text>
        <text x="4.5" y="1.8" font-size="0.3">(5,4)</text>
        <text x="3.5" y="2.8" font-size="0.3">(4,3)</text>
        <text x="5.5" y="2.8" font-size="0.3">(6,3)</text>
        <text x="6.5" y="0.8" font-size="0.3">(7,5)</text>
        <text x="8.5" y="0.8" font-size="0.3">(9,5)</text>
        <text x="7.5" y="3.8" font-size="0.3">(8,2)</text>
        <text x="9.5" y="3.8" font-size="0.3">(10,2)</text>
    
        <!-- skyline contour -->
        <polyline
          points="
            1,4   2,4
            2,2   5,2
            5,3   6,3
            6,6   7,6
            7,1   9,1
            9,4  10,4
            10,6"
          fill="none" stroke="black" stroke-width="0.05"/>
        <g id="skyline-keypoints" fill="black">
          <circle cx="1"  cy="4" r="0.1"/>
          <circle cx="2"  cy="2" r="0.1"/>
          <circle cx="5"  cy="3" r="0.1"/>
          <circle cx="6"  cy="6" r="0.1"/>
          <circle cx="7"  cy="1" r="0.1"/>
          <circle cx="9"  cy="4" r="0.1"/>
          <circle cx="10" cy="6" r="0.1"/>
        </g>
            <!-- x-axis -->
            <line x1="0" y1="6" x2="10" y2="6" stroke="black" stroke-width="0.02"/>
            <!-- ticks and labels -->
            <g id="y-axis" stroke="black" stroke-width="0.02" fill="black" font-size="0.3" text-anchor="end">
          <!-- axis line -->
          <line x1="0" y1="6" x2="0" y2="1"/>
          <!-- ticks and labels -->
          <line x1="0" y1="5" x2="-0.2" y2="5"/><text x="-0.3" y="5" alignment-baseline="middle">1</text>
          <line x1="0" y1="4" x2="-0.2" y2="4"/><text x="-0.3" y="4" alignment-baseline="middle">2</text>
          <line x1="0" y1="3" x2="-0.2" y2="3"/><text x="-0.3" y="3" alignment-baseline="middle">3</text>
          <line x1="0" y1="2" x2="-0.2" y2="2"/><text x="-0.3" y="2" alignment-baseline="middle">4</text>
          <line x1="0" y1="1" x2="-0.2" y2="1"/><text x="-0.3" y="1" alignment-baseline="middle">5</text>
        </g>
        <g stroke="black" stroke-width="0.02">
          <line x1="1" y1="6" x2="1" y2="5.8"/><text x="1" y="6.3" font-size="0.3" text-anchor="middle">1</text>
          <line x1="2" y1="6" x2="2" y2="5.8"/><text x="2" y="6.3" font-size="0.3" text-anchor="middle">2</text>
          <line x1="3" y1="6" x2="3" y2="5.8"/><text x="3" y="6.3" font-size="0.3" text-anchor="middle">3</text>
          <line x1="4" y1="6" x2="4" y2="5.8"/><text x="4" y="6.3" font-size="0.3" text-anchor="middle">4</text>
          <line x1="5" y1="6" x2="5" y2="5.8"/><text x="5" y="6.3" font-size="0.3" text-anchor="middle">5</text>
          <line x1="6" y1="6" x2="6" y2="5.8"/><text x="6" y="6.3" font-size="0.3" text-anchor="middle">6</text>
          <line x1="7" y1="6" x2="7" y2="5.8"/><text x="7" y="6.3" font-size="0.3" text-anchor="middle">7</text>
          <line x1="8" y1="6" x2="8" y2="5.8"/><text x="8" y="6.3" font-size="0.3" text-anchor="middle">8</text>
          <line x1="9" y1="6" x2="9" y2="5.8"/><text x="9" y="6.3" font-size="0.3" text-anchor="middle">9</text>
          <line x1="10" y1="6" x2="10" y2="5.8"/><text x="10" y="6.3" font-size="0.3" text-anchor="middle">10</text>
        </g>
      </svg>
      </blockquote> 
      <ol type="a">
        <li>Describe a straightforward algorithm and give its worst-case complexity.</li>
        <li>Devise a divide-and-conquer algorithm: specify how you split the building set, 
        how you merge two skylines, and why merge takes \(O(n)\) time.</li>
        <li>Derive and solve the recurrence for your divide-and-conquer algorithm.</li>
        <li>(Optional) Describe the sweep-line + max-heap approach and explain why it has
        a worst-case complexity of \(O(n \log n)\).</li>
        <li>Compare the approaches in terms of complexity and implementation difficulty.</li>
      </ol>
    </li>
  
    <li>
      <strong>Multi-Way Merge</strong><br>
      You have \(k\) sorted lists containing a total of \(n\) elements; merge them into one sorted list.
      <ol type="a">
        <li>Describe the naive strategy and derive its worst-case complexity.</li>
        <li>Devise a divide-and-conquer algorithm: explain how you split the \(k\) lists, 
        how you merge results, and why each merge is \(O(n)\).</li>
        <li>Write and solve the recurrence for the worst-case complexity of your divide-and-conquer 
        algorithm in terms of \(n\) and \(k\).</li>
        <li>Describe the heap-based approach using a min-heap of size \(k\) and 
        show that its worst-case complexity is \(O(n \log k)\) time.</li>
        <li>Compare the three methods in terms of implementation and complexity.</li>
      </ol>
    </li>
    <li>
  <strong>Counting Line-Segment Intersections</strong><br>
  You are given an integer \(n\) and a list of \(n\) line segments in the plane.  
  Each segment is specified by its two endpoints  
  \((x_{i1},y_{i1})\) and\((x_{i2},y_{i2})\),
  where all coordinates are real numbers (or integers), and no segment has zero length.  
  You may assume no three segments meet at a single point.
  The goal is to count the number of unordered pairs \(\{i,j\}\) with \(1\le i\lt j\le n\) whose corresponding segments intersect at a point (including interior or endpoint intersections).

  <ol type="a">
    <li>
      Describe a brute-force/exhaustive search algorithm and determine its worst-case time complexity.
    </li>
    <li>(Difficult)
      Devise a divide-and-conquer strategy to solve the problem. 
      <ol type='i'>
          <li>Explain how you split by a vertical line, 
          how you count within each half recursively, and how you count "cross-line" intersections.</li>
        <li>Explain why counting cross intersections costs \(O(n\log n)\).</li>
        <li>Explain why the worst-case complexity of the algorithm is given by the recurrence  
              \(\;T(n)=2\,T(n/2)+O(n\log n)\), and show that the closed form is  \(T(n)=O(n\log^2 n)\).</li>
        <li>Suggest any improvements or alternative strategies that are more efficient.</li>
      </ol>
      </ol>
    </li>
      <li>
        <strong>Median-of-Medians Selection:</strong><br>
        Find a reference that discusses the Median-of-Medians Selection algorithm.
        In your own words, explain how the algorithm works.
        Give pseudocode for the algorithm that gives enough detail that you are able to do a complete
        complexity analysis of the algorithm.
        Derive the recurrence \(T(n)=T(n/5)+T(7n/10)+O(n)\) for its worst-case compliexity 
        and show that \(T(n)=O(n)\).
      </li>
  </ol>
</section>
  
</section>


</body>
</html>
