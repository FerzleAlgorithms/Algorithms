<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Matrix Multiplication (Divide and Conquer)</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="/Algorithms/scripts/chapterScripts.js"></script>
  <link rel="stylesheet" href="/Algorithms/css/style.css">
  <link rel="stylesheet" href="/Algorithms/css/chapter.css">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DQ5LVZVFDC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-DQ5LVZVFDC');
  </script>
</head>
<body>
  <h1>Matrix Multiplication (Divide and Conquer)</h1>

  <section id="problem-solved" section-title="Problem Solved">
  <h2>Problem Solved</h2>
  <p>
    Matrix Multiplication (Divide and Conquer) solves the
    <a class="problem" href="?path=Problems%2FFoundational%2FMatrix%20Multiplication">Matrix Multiplication</a>
    problem using a divide-and-conquer approach. Although it turns out to not be more efficient than the brute-force algorithm we will see later, it provides a foundation for more advanced algorithms like Strassen's method. The divide-and-conquer strategy allows us to break down the problem of multiplying two \(n\times n\) matrices into smaller subproblems, which can be solved recursively.
  </p>
  </section>

  <section id="design" section-title="Design and Strategy">
  <h2>Design and Strategy</h2>
  <p>
    The divide-and-conquer approach to matrix multiplication recursively breaks down the problem of multiplying two \(n\times n\) matrices into smaller subproblems. Instead of using the standard \(O(n^3)\) algorithm that computes each entry using a dot product, we divide each matrix into four quadrants and use the mathematical property that matrix multiplication can be expressed in terms of operations on these quadrants.
  </p>
  
  <p><strong>Size Restriction:</strong> For simplicity, we assume \(n = 2^k\) for some integer \(k\), so matrices can be evenly divided. This ensures each recursive call works with matrices of size \(n/2 \times n/2\). Matrices of odd sizes or non-power-of-2 dimensions can be handled by padding with zeros to the next power of 2, though this introduces some overhead.</p>

  <p>
    For two \(n\times n\) matrices A and B, we partition them into four \(n/2 \times n/2\) submatrices each:
    \[A = \begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix}, \quad B = \begin{pmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{pmatrix}\]
    where each \(A_{ij}\) and \(B_{ij}\) is an \(n/2 \times n/2\) matrix.
  </p>
  
  <p>
    The product C = A × B can then be computed as:
    \[C = \begin{pmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \end{pmatrix}\]
    where each \(C_{ij}\) is also an \(n/2 \times n/2\) matrix given by:
    \[C_{11} = A_{11}B_{11} + A_{12}B_{21}\]
    \[C_{12} = A_{11}B_{12} + A_{12}B_{22}\]
    \[C_{21} = A_{21}B_{11} + A_{22}B_{21}\]
    \[C_{22} = A_{21}B_{12} + A_{22}B_{22}\]
  </p>

  <section id="whyitworks" section-title="Why This Formula Works">
  <h3>Why This Formula Works</h3> 
    The key insight is that matrix multiplication follows the same algebraic rules whether we think of entries as scalars or as blocks (submatrices). To see why, recall that in standard matrix multiplication, entry \(C[i][j]\) is computed as:
  \[C[i][j] = \sum_{k=0}^{n-1} A[i][k] \cdot B[k][j]\]
  
  When we partition the matrices, this sum naturally splits based on which quadrant the indices fall into:</p>
  
  <ul>
    <li><strong>For \(C_{11}\):</strong> Entries where both \(i < n/2\) and \(j < n/2\). The sum decomposes as:
    \begin{align}
    C[i][j] &= \sum_{k=0}^{n-1} A[i][k] \cdot B[k][j] \\
    &= \sum_{k=0}^{n/2-1} A[i][k] \cdot B[k][j] + \sum_{k=n/2}^{n-1} A[i][k] \cdot B[k][j] \\
    &= (A_{11} \times B_{11})[i][j] + (A_{12} \times B_{21})[i][j]
    \end{align}
    The first sum uses the left half of row \(i\) from \(A\) with the top half of column \(j\) from \(B\), giving us the contribution from \(A_{11} \times B_{11}\). The second sum uses the right half of row \(i\) from \(A\) with the bottom half of column \(j\) from \(B\), giving us \(A_{12} \times B_{21}\).</li>
    
    <li><strong>For \(C_{12}\):</strong> Entries where \(i < n/2\) and \(j \geq n/2\). The sum decomposes as:
    \begin{align}
    C[i][j] &= \sum_{k=0}^{n-1} A[i][k] \cdot B[k][j] \\
    &= \sum_{k=0}^{n/2-1} A[i][k] \cdot B[k][j] + \sum_{k=n/2}^{n-1} A[i][k] \cdot B[k][j] \\
    &= (A_{11} \times B_{12})[i][j] + (A_{12} \times B_{22})[i][j]
    \end{align}
    Here we're using rows from the top half of \(A\) with columns from the right half of \(B\). The first sum gives us \(A_{11} \times B_{12}\) and the second gives us \(A_{12} \times B_{22}\).</li>
    
    <li><strong>Similarly:</strong> \(C_{21}\) and \(C_{22}\) follow the same pattern. For \(C_{21}\) (bottom-left quadrant), we get \(C_{21} = A_{21}B_{11} + A_{22}B_{21}\), and for \(C_{22}\) (bottom-right quadrant), we get \(C_{22} = A_{21}B_{12} + A_{22}B_{22}\).</li>
  </ul>
  </section>

  <section id="example" section-title="Examples">
  <h3>Examples</h3> 
  <div class="example-box">
    <strong class="example-title">Detailed Example: 4×4 Matrix Multiplication</strong>
    <p>Let's multiply two 4×4 matrices using both approaches to see they produce identical results:</p>
    
    <p>
      \[A = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \\ 13 & 14 & 15 & 16 \end{pmatrix}, \quad B = \begin{pmatrix} 2 & 1 & 3 & 0 \\ 1 & 2 & 0 & 1 \\ 0 & 1 & 2 & 3 \\ 3 & 0 & 1 & 2 \end{pmatrix}\]
    </p>
    
    <p><strong>Step 1: Divide-and-Conquer Partitioning</strong></p>
    <p>First, we partition each matrix into four 2×2 submatrices:</p>
    
    <p>
      \[A_{11} = \begin{pmatrix} 1 & 2 \\ 5 & 6 \end{pmatrix}, \quad A_{12} = \begin{pmatrix} 3 & 4 \\ 7 & 8 \end{pmatrix}\]
      \[A_{21} = \begin{pmatrix} 9 & 10 \\ 13 & 14 \end{pmatrix}, \quad A_{22} = \begin{pmatrix} 11 & 12 \\ 15 & 16 \end{pmatrix}\]
    </p>
    
    <p>
      \[B_{11} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \quad B_{12} = \begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix}\]
      \[B_{21} = \begin{pmatrix} 0 & 1 \\ 3 & 0 \end{pmatrix}, \quad B_{22} = \begin{pmatrix} 2 & 3 \\ 1 & 2 \end{pmatrix}\]
    </p>
    
    <p><strong>Step 2: Compute \(C_{11} = A_{11}B_{11} + A_{12}B_{21}\)</strong></p>
    
    <p>First, \(A_{11}B_{11}\):</p>
    <p>\[\begin{pmatrix} 1 & 2 \\ 5 & 6 \end{pmatrix} \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} = \begin{pmatrix} 1(2)+2(1) & 1(1)+2(2) \\ 5(2)+6(1) & 5(1)+6(2) \end{pmatrix} = \begin{pmatrix} 4 & 5 \\ 16 & 17 \end{pmatrix}\]</p>
    
    <p>Next, \(A_{12}B_{21}\):</p>
    <p>\[\begin{pmatrix} 3 & 4 \\ 7 & 8 \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 3 & 0 \end{pmatrix} = \begin{pmatrix} 3(0)+4(3) & 3(1)+4(0) \\ 7(0)+8(3) & 7(1)+8(0) \end{pmatrix} = \begin{pmatrix} 12 & 3 \\ 24 & 7 \end{pmatrix}\]</p>
    
    <p>Therefore: \(C_{11} = \begin{pmatrix} 4 & 5 \\ 16 & 17 \end{pmatrix} + \begin{pmatrix} 12 & 3 \\ 24 & 7 \end{pmatrix} = \begin{pmatrix} 16 & 8 \\ 40 & 24 \end{pmatrix}\)</p>
    
    <p><strong>Step 3: Compute \(C_{12} = A_{11}B_{12} + A_{12}B_{22}\)</strong></p>
    
    <p>\(A_{11}B_{12} = \begin{pmatrix} 1 & 2 \\ 5 & 6 \end{pmatrix} \begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 3 & 2 \\ 15 & 6 \end{pmatrix}\)</p>
    
    <p>\(A_{12}B_{22} = \begin{pmatrix} 3 & 4 \\ 7 & 8 \end{pmatrix} \begin{pmatrix} 2 & 3 \\ 1 & 2 \end{pmatrix} = \begin{pmatrix} 10 & 17 \\ 22 & 37 \end{pmatrix}\)</p>
    
    <p>Therefore: \(C_{12} = \begin{pmatrix} 3 & 2 \\ 15 & 6 \end{pmatrix} + \begin{pmatrix} 10 & 17 \\ 22 & 37 \end{pmatrix} = \begin{pmatrix} 13 & 19 \\ 37 & 43 \end{pmatrix}\)</p>
    
    <p><strong>Step 4: Compute \(C_{21} = A_{21}B_{11} + A_{22}B_{21}\)</strong></p>
    
    <p>\(A_{21}B_{11} = \begin{pmatrix} 9 & 10 \\ 13 & 14 \end{pmatrix} \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} = \begin{pmatrix} 28 & 29 \\ 40 & 41 \end{pmatrix}\)</p>
    
    <p>\(A_{22}B_{21} = \begin{pmatrix} 11 & 12 \\ 15 & 16 \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 3 & 0 \end{pmatrix} = \begin{pmatrix} 36 & 11 \\ 48 & 15 \end{pmatrix}\)</p>
    
    <p>Therefore: \(C_{21} = \begin{pmatrix} 28 & 29 \\ 40 & 41 \end{pmatrix} + \begin{pmatrix} 36 & 11 \\ 48 & 15 \end{pmatrix} = \begin{pmatrix} 64 & 40 \\ 88 & 56 \end{pmatrix}\)</p>
    
    <p><strong>Step 5: Compute \(C_{22} = A_{21}B_{12} + A_{22}B_{22}\)</strong></p>
    
    <p>\(A_{21}B_{12} = \begin{pmatrix} 9 & 10 \\ 13 & 14 \end{pmatrix} \begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 27 & 10 \\ 39 & 14 \end{pmatrix}\)</p>
    
    <p>\(A_{22}B_{22} = \begin{pmatrix} 11 & 12 \\ 15 & 16 \end{pmatrix} \begin{pmatrix} 2 & 3 \\ 1 & 2 \end{pmatrix} = \begin{pmatrix} 34 & 57 \\ 46 & 77 \end{pmatrix}\)</p>
    
    <p>Therefore: \(C_{22} = \begin{pmatrix} 27 & 10 \\ 39 & 14 \end{pmatrix} + \begin{pmatrix} 34 & 57 \\ 46 & 77 \end{pmatrix} = \begin{pmatrix} 61 & 67 \\ 85 & 91 \end{pmatrix}\)</p>
    
    <p><strong>Final Divide-and-Conquer Result:</strong></p>
    <p>\[C = \begin{pmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \end{pmatrix} = \begin{pmatrix} 16 & 8 & 13 & 19 \\ 40 & 24 & 37 & 43 \\ 64 & 40 & 61 & 67 \\ 88 & 56 & 85 & 91 \end{pmatrix}\]</p>
  </div>

  <div class="example-box">
    <strong class="example-title">Verification: Brute Force Computation</strong>
    <p>Let's verify our result by computing the same multiplication using the standard \(O(n^3)\) algorithm. For each entry \(C[i][j] = \sum_{k=0}^{3} A[i][k] \cdot B[k][j]\):</p>
    
    <p><strong>First Row (i=0):</strong></p>
    <p>\(C[0][0] = 1(2) + 2(1) + 3(0) + 4(3) = 2 + 2 + 0 + 12 = 16\) ✓</p>
    <p>\(C[0][1] = 1(1) + 2(2) + 3(1) + 4(0) = 1 + 4 + 3 + 0 = 8\) ✓</p>
    <p>\(C[0][2] = 1(3) + 2(0) + 3(2) + 4(1) = 3 + 0 + 6 + 4 = 13\) ✓</p>
    <p>\(C[0][3] = 1(0) + 2(1) + 3(3) + 4(2) = 0 + 2 + 9 + 8 = 19\) ✓</p>
    
    <p><strong>Second Row (i=1):</strong></p>
    <p>\(C[1][0] = 5(2) + 6(1) + 7(0) + 8(3) = 10 + 6 + 0 + 24 = 40\) ✓</p>
    <p>\(C[1][1] = 5(1) + 6(2) + 7(1) + 8(0) = 5 + 12 + 7 + 0 = 24\) ✓</p>
    <p>\(C[1][2] = 5(3) + 6(0) + 7(2) + 8(1) = 15 + 0 + 14 + 8 = 37\) ✓</p>
    <p>\(C[1][3] = 5(0) + 6(1) + 7(3) + 8(2) = 0 + 6 + 21 + 16 = 43\) ✓</p>
    
    <p><strong>Third Row (i=2):</strong></p>
    <p>\(C[2][0] = 9(2) + 10(1) + 11(0) + 12(3) = 18 + 10 + 0 + 36 = 64\) ✓</p>
    <p>\(C[2][1] = 9(1) + 10(2) + 11(1) + 12(0) = 9 + 20 + 11 + 0 = 40\) ✓</p>
    <p>\(C[2][2] = 9(3) + 10(0) + 11(2) + 12(1) = 27 + 0 + 22 + 12 = 61\) ✓</p>
    <p>\(C[2][3] = 9(0) + 10(1) + 11(3) + 12(2) = 0 + 10 + 33 + 24 = 67\) ✓</p>
    
    <p><strong>Fourth Row (i=3):</strong></p>
    <p>\(C[3][0] = 13(2) + 14(1) + 15(0) + 16(3) = 26 + 14 + 0 + 48 = 88\) ✓</p>
    <p>\(C[3][1] = 13(1) + 14(2) + 15(1) + 16(0) = 13 + 28 + 15 + 0 = 56\) ✓</p>
    <p>\(C[3][2] = 13(3) + 14(0) + 15(2) + 16(1) = 39 + 0 + 30 + 16 = 85\) ✓</p>
    <p>\(C[3][3] = 13(0) + 14(1) + 15(3) + 16(2) = 0 + 14 + 45 + 32 = 91\) ✓</p>
    
    <p><strong>Both methods produce identical results!</strong> This confirms that our divide-and-conquer block formula correctly implements matrix multiplication by preserving the fundamental row-column dot product computation, just organized into larger blocks.</p>
  </div>
  </section>

  <section id="pseudocode" section-title="Pseudocode">
    <h3>Pseudocode</h3>

  <p>This idea leads the pseudocode like the following:</p>
  <ol>
    <li><strong>Base case:</strong> If current matrix size is smaller than some threshold, 
      use standard nested-loop multiplication</li>
    <li><strong>Recursive case:</strong> Divide current matrices into four quadrants of size \(n/2 \times n/2\)</li>
    <li><strong>Compute each result quadrant by combining two recursive products:</strong>
      <ul>
        <li><em>\(C_{11}:\)</em> Recursively compute \(A_{11} \times B_{11}\) and \(A_{12} \times B_{21}\), then add the results to get \(C_{11}\)</li>
        <li><em>\(C_{12}:\)</em> Recursively compute \(A_{11} \times B_{12}\) and \(A_{12} \times B_{22}\), then add the results to get \(C_{12}\)</li>
        <li><em>\(C_{21}:\)</em> Recursively compute \(A_{21} \times B_{11}\) and \(A_{22} \times B_{21}\), then add the results to get \(C_{21}\)</li>
        <li><em>\(C_{22}:\)</em> Recursively compute \(A_{21} \times B_{12}\) and \(A_{22} \times B_{22}\), then add the results to get \(C_{22}\)</li>
      </ul>
    </li>
  </ol>
  
  <p><strong>Key Features:</strong></p>
  <ul>
    <li>Makes 8 recursive calls total (2 for each result quadrant) with matrices of half the size</li>
    <li>Results accumulate directly into the final matrix C</li>
    <li>Can be implemented to avoid temporary matrices by working with subregions of the matrices</li>
    <li>Switches to standard O(n³) algorithm below threshold to avoid excessive recursion overhead</li>
  </ul>

  <p>The following interactive demonstration shows how the algorithm works step by step. Make sure to run
    it with \(n=8\) to get a better taste of how it works recursively.
  </p>
  </section>

  <section id="demo" section-title="Interactive Demos">
  <div class="embeddedDemoContainer">
    <iframe class="embeddedDemo"
            src="/Algorithms/Content/Demos/Divide-and-Conquer/Matrix Multiplication Demo.html"
            allow="fullscreen"
            name="matrix-multiplication-demo">
    </iframe>
  </div>
  <p>
    Although that demo captures the essence of the algorithm, it is not how one would actually implement 
    it in practice. The second version of the demo performs computations in-place, avoiding extra space 
    for submatrices. Instead of creating submatrices for each recursive call and then copying results back, 
    the algorithm passes offsets and operates directly on the relevant regions of the original matrices. 
    For example, rather than creating two submatrices and then adding them, the first computation updates 
    the corresponding submatrix of the result, and the second adds to those entries. 
    This one is harder to follow, but if you understand the concepts from the previous one, 
    it should help this one make sense. Check it out:
  </p>
  <div class="embeddedDemoContainer">
    <iframe class="embeddedDemo"
            src="/Algorithms/Content/Demos/Divide-and-Conquer/Matrix Multiplcation (Inplace) Demo.html"
            allow="fullscreen"
            name="matrix-multiplication-demo2">
    </iframe>
  </div>
  </section>
  


  <section id="code" section-title="Implementation in Java, C++, Python">
  <h2>Implementation in Java, C++, Python</h2>
  <p>
    <strong>Key Implementation Details:</strong> This algorithm is a bit different from most you have seen before. Instead of working with whole matrices or copying out submatrices for each recursive call, we operate directly on subregions of the original matrices by passing <em>offsets</em>—that is, the row and column indices marking the top-left corner of the submatrix we want to work on. This is similar in spirit to how quicksort works: instead of copying out the left and right halves of the array, quicksort just passes the indices for the left and right boundaries to each recursive call. Here, we do the same for two-dimensional matrices, specifying which part of each matrix to use for each subproblem.
  </p>
  <p>
    For each recursive call, we specify the starting row and column for the submatrix in <code>A</code>, <code>B</code> and <code>C</code> (the result), along with the size of the submatrix. For example, <code>aRow</code> and <code>aCol</code> specify the top-left corner of the current submatrix of <code>A</code> that we are working with, and similarly for <code>B</code> and <code>C</code>. The <code>size</code> parameter tells us how large the current submatrix is (always a square of <code>size × size</code>).
  </p>
  <p>
    This offset-based approach is crucial for efficiency: if we were to copy out submatrices at each level of recursion, the overhead would be enormous—both in time and space. By working with offsets, we avoid unnecessary copying and work directly with the original data.
  </p>
  <p>
    <strong>How the Recursive Calls Work:</strong> Each recursive call is responsible for computing a specific quadrant of the result matrix <code>C</code>. To do this, it combines two products of submatrices from <code>A</code> and <code>B</code>, each specified by their own offsets. The results are accumulated directly into the appropriate region of <code>C</code>. This is why, for each quadrant, we make two recursive calls: the first writes to the region, and the second adds to it.
  </p>
  <p>
    <strong>Cutoff for Efficiency:</strong> In practice, a cutoff (or threshold) is often used: when the submatrix size becomes small (for example, 64×64 or smaller), the algorithm switches to the standard triple-nested-loop method. This is a common optimization in recursive algorithms to reduce overhead for small subproblems. However, as we will discuss later, for this particular algorithm the cutoff does not actually change the overall efficiency or asymptotic behavior, since the divide-and-conquer approach itself is not faster than the standard method.
  </p>
  <div class="tab-group">
    <div class="tabs" role="tablist">
      <button id="tab-java" class="tablink active" data-lang="java" role="tab" aria-controls="java" aria-selected="true">Java</button>
      <button id="tab-cpp" class="tablink" data-lang="cpp" role="tab" aria-controls="cpp" aria-selected="false">C++</button>
      <button id="tab-python" class="tablink" data-lang="python" role="tab" aria-controls="python" aria-selected="false">Python</button>
    </div>
    <div id="java" class="code-container active" role="tabpanel" aria-labelledby="tab-java">
      <pre><code class="language-java">// No class wrapper needed for illustration
static final int THRESHOLD = 64;

/**
 * Multiplies two n×n matrices using divide-and-conquer.
 * Assumes n is a power of 2 for simplicity.
 */
public static int[][] multiply(int[][] A, int[][] B) {
    int n = A.length;
    int[][] C = new int[n][n];
    multiplyRecursive(A, B, C, 0, 0, 0, 0, 0, 0, n);
    return C;
}

/**
 * Recursive helper method that multiplies submatrices using offsets.
 */
private static void multiplyRecursive(int[][] A, int[][] B, int[][] C,
                                     int aRow, int aCol, int bRow, int bCol,
                                     int cRow, int cCol, int size) {
    if (size &lt;= THRESHOLD) {
        // Base case: use standard O(n³) multiplication
        for (int i = 0; i &lt; size; i++) {
            for (int j = 0; j &lt; size; j++) {
                for (int k = 0; k &lt; size; k++) {
                    C[cRow + i][cCol + j] += A[aRow + i][aCol + k] * B[bRow + k][bCol + j];
                }
            }
        }
        return;
    }
    int newSize = size / 2;
    // C11 = A11*B11 + A12*B21
    multiplyRecursive(A, B, C, aRow, aCol, bRow, bCol, cRow, cCol, newSize);
    multiplyRecursive(A, B, C, aRow, aCol + newSize, bRow + newSize, bCol, cRow, cCol, newSize);
    // C12 = A11*B12 + A12*B22
    multiplyRecursive(A, B, C, aRow, aCol, bRow, bCol + newSize, cRow, cCol + newSize, newSize);
    multiplyRecursive(A, B, C, aRow, aCol + newSize, bRow + newSize, bCol + newSize, cRow, cCol + newSize, newSize);
    // C21 = A21*B11 + A22*B21
    multiplyRecursive(A, B, C, aRow + newSize, aCol, bRow, bCol, cRow + newSize, cCol, newSize);
    multiplyRecursive(A, B, C, aRow + newSize, aCol + newSize, bRow + newSize, bCol, cRow + newSize, cCol, newSize);
    // C22 = A21*B12 + A22*B22
    multiplyRecursive(A, B, C, aRow + newSize, aCol, bRow, bCol + newSize, cRow + newSize, cCol + newSize, newSize);
    multiplyRecursive(A, B, C, aRow + newSize, aCol + newSize, bRow + newSize, bCol + newSize, cRow + newSize, cCol + newSize, newSize);
}
</code></pre>
    </div>
    <div id="cpp" class="code-container" role="tabpanel" aria-labelledby="tab-cpp">
      <pre><code class="language-cpp">// No class or #include needed for illustration
const int THRESHOLD = 64;

// Multiply two n x n matrices using divide-and-conquer
void multiply(const std::vector&lt;std::vector&lt;int&gt;&gt;&amp; A,
              const std::vector&lt;std::vector&lt;int&gt;&gt;&amp; B,
              std::vector&lt;std::vector&lt;int&gt;&gt;&amp; C,
              int aRow, int aCol, int bRow, int bCol,
              int cRow, int cCol, int size) {
    if (size &lt;= THRESHOLD) {
        for (int i = 0; i &lt; size; i++)
            for (int j = 0; j &lt; size; j++)
                for (int k = 0; k &lt; size; k++)
                    C[cRow + i][cCol + j] += A[aRow + i][aCol + k] * B[bRow + k][bCol + j];
        return;
    }
    int newSize = size / 2;
    // C11 = A11*B11 + A12*B21
    multiply(A, B, C, aRow, aCol, bRow, bCol, cRow, cCol, newSize);
    multiply(A, B, C, aRow, aCol + newSize, bRow + newSize, bCol, cRow, cCol, newSize);
    // C12 = A11*B12 + A12*B22
    multiply(A, B, C, aRow, aCol, bRow, bCol + newSize, cRow, cCol + newSize, newSize);
    multiply(A, B, C, aRow, aCol + newSize, bRow + newSize, bCol + newSize, cRow, cCol + newSize, newSize);
    // C21 = A21*B11 + A22*B21
    multiply(A, B, C, aRow + newSize, aCol, bRow, bCol, cRow + newSize, cCol, newSize);
    multiply(A, B, C, aRow + newSize, aCol + newSize, bRow + newSize, bCol, cRow + newSize, cCol, newSize);
    // C22 = A21*B12 + A22*B22
    multiply(A, B, C, aRow + newSize, aCol, bRow, bCol + newSize, cRow + newSize, cCol + newSize, newSize);
    multiply(A, B, C, aRow + newSize, aCol + newSize, bRow + newSize, bCol + newSize, cRow + newSize, cCol + newSize, newSize);
}

// To call: 
// std::vector&lt;std::vector&lt;int&gt;&gt; C(n, std::vector&lt;int&gt;(n, 0));
// multiply(A, B, C, 0, 0, 0, 0, 0, 0, n);
</code></pre>
    </div>
    <div id="python" class="code-container" role="tabpanel" aria-labelledby="tab-python">
      <pre><code class="language-python">THRESHOLD = 64

def multiply(A, B):
    """
    Multiply two n x n matrices using divide-and-conquer.
    Assumes n is a power of 2.
    """
    n = len(A)
    C = [[0] * n for _ in range(n)]
    _multiply_recursive(A, B, C, 0, 0, 0, 0, 0, 0, n)
    return C

def _multiply_recursive(A, B, C, a_row, a_col, b_row, b_col, c_row, c_col, size):
    if size <= THRESHOLD:
        for i in range(size):
            for j in range(size):
                for k in range(size):
                    C[c_row + i][c_col + j] += A[a_row + i][a_col + k] * B[b_row + k][b_col + j]
        return
    new_size = size // 2
    # C11 = A11*B11 + A12*B21
    _multiply_recursive(A, B, C, a_row, a_col, b_row, b_col, c_row, c_col, new_size)
    _multiply_recursive(A, B, C, a_row, a_col + new_size, b_row + new_size, b_col, cRow, cCol, newSize)
    # C12 = A11*B12 + A12*B22
    _multiply_recursive(A, B, C, a_row, a_col, b_row, b_col + new_size, c_row, c_col + new_size, new_size)
    _multiply_recursive(A, B, C, aRow, aCol + newSize, bRow + newSize, bCol + newSize, cRow, cCol + newSize, new_size)
    # C21 = A21*B11 + A22*B21
    _multiply_recursive(A, B, C, a_row + new_size, a_col, b_row, b_col, c_row + new_size, c_col, new_size)
    _multiply_recursive(A, B, C, a_row + new_size, a_col + new_size, b_row + new_size, b_col, c_row + new_size, c_col, new_size)
    # C22 = A21*B12 + A22*B22
    _multiply_recursive(A, B, C, a_row + new_size, a_col, b_row, b_col + new_size, c_row + new_size, c_col + new_size, new_size)
    _multiply_recursive(A, B, C, a_row + new_size, a_col + new_size, b_row + new_size, b_col + new_size, c_row + new_size, c_col + new_size, new_size)
</code></pre>
    </div>
  </div>
  </section>

  <section id="analysis" section-title="Time/Space Analysis">
  <h2>Time/Space Analysis</h2>
  <p>
    <strong>Time Complexity:</strong> The recurrence relation for the running time of this divide-and-conquer algorithm is:
    \[
      T(n) = 8T(n/2) + O(n^2)
    \]
    where the \(8T(n/2)\) term comes from the eight recursive multiplications of submatrices of size \(n/2 \times n/2\), and the \(O(n^2)\) term comes from the additions needed to combine results. By the Master Theorem (\(a = 8\), \(b = 2\), \(f(n) = O(n^2)\)), we have \(n^{\log_2 8} = n^3\), so the solution is \(T(n) = O(n^3)\).
  </p>
  <p>
    This is the same asymptotic complexity as the brute-force (triple-loop) algorithm, but in practice, the divide-and-conquer approach actually incurs more overhead due to recursion and function calls. Thus, it is not used for performance on its own.
  </p>
  <p>
    However, this recursive structure is the foundation for more advanced algorithms such as Strassen's algorithm, which reduces the number of recursive multiplications and achieves a better asymptotic running time. Strassen's algorithm, for example, uses a recurrence of the form \(T(n) = 7T(n/2) + O(n^2)\), leading to a time complexity of \(O(n^{\log_2 7}) \approx O(n^{2.81})\).
  </p>
  <p>
    <strong>Operation Counts:</strong> For this basic divide-and-conquer algorithm, the number of multiplications and additions can be described by the following recurrences:
    <ul>
      <li>Multiplications: \(M(n) = 8M(n/2)\), with \(M(1) = 1\), so \(M(n) = n^3\).</li>
      <li>Additions: \(A(n) = 8A(n/2) + 4(n/2)^2\), with \(A(1) = 0\), which solves to \(A(n) = n^3 - n^2\).</li>
    </ul>
    These are exactly the same as the brute-force algorithm. In contrast, Strassen's algorithm reduces the number of multiplications at the cost of more additions, which is why these counts are important for comparison.
  </p>
  <p>
    <strong>Space Complexity:</strong> \(O(\log n)\) for the recursion stack (since we recurse \(\log_2 n\) levels deep), plus \(O(n^2)\) for the result matrix. The key advantage is that we don't create temporary submatrices at each level—we work directly with the original arrays using offsets.
  </p>
  <p>
    <strong>Handling Non-Power-of-2 Sizes:</strong> For matrices where \(n\) is not a power of 2, we can pad the matrices with zeros to reach the next power of 2, then trim the result. This adds some overhead but allows the algorithm to work with any size. Alternatively, more complex implementations can handle odd sizes by carefully managing the base cases.
  </p>
  </section>
  
  
  <section id="variations" section-title="Variations/Improvements">
  <h2>Variations/Improvements</h2>
  <p>
    <strong>Strassen's Algorithm:</strong> The first major improvement over the basic divide-and-conquer approach, Strassen's algorithm reduces the number of recursive multiplications from 8 to 7 by cleverly reorganizing the computation. This changes the recurrence to \(T(n) = 7T(n/2) + O(n^2)\), yielding a time complexity of about \(O(n^{2.81})\). Strassen's method achieves this by trading some multiplications for more additions, which is worthwhile because additions are much cheaper than multiplications for large matrices. Strassen's algorithm is the first example of a divide-and-conquer matrix multiplication that is actually faster than the standard cubic-time approach.
  </p>
  <p>
    <strong>Further Fast Algorithms:</strong> There are even faster theoretical algorithms (such as the Coppersmith-Winograd algorithm and its successors) that achieve better asymptotic complexity, but they are rarely used in practice due to large constant factors and implementation complexity.
  </p>
  <p>
    <strong>Cache-Oblivious and Practical Improvements:</strong> Cache-oblivious algorithms use divide-and-conquer to improve memory access patterns, making them efficient on modern hardware by reducing cache misses. These approaches can provide significant speedups in practice, even if their asymptotic complexity is not better.
  </p>
  <p>
    <strong>Parallelization:</strong> The recursive structure of divide-and-conquer matrix multiplication naturally supports parallelization. Different subproblems (quadrants) can be computed independently, allowing the algorithm to take advantage of multiple cores or distributed systems.
  </p>
  </section>

  <section id="links" section-title="Links to Resources">
  <h2>Links to Resources</h2>
  <ul>
    <li><a href="https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm" target="_blank">Matrix Multiplication Algorithms (Wikipedia)</a> Comprehensive overview of various matrix multiplication approaches, including the divide-and-conquer approach</li>
    <li><a href="https://www.geeksforgeeks.org/dsa/strassens-matrix-multiplication/" target="_blank">Matrix Multiplication (GeeksforGeeks)</a> Implementation details for several matrix multiplication algorithms</li>
  </ul>
  </section>

 <section id="reading-questions" section-title="Reading Comprehension Questions">
  <h2>Reading Comprehension Questions</h2>
  <ol>
    <li><strong>Recurrence and Complexity:</strong> What is the recurrence relation for the running time of the basic divide-and-conquer matrix multiplication algorithm? Use the Master Theorem to solve it and state the resulting time complexity.</li>

    <li><strong>Comparison to Brute Force:</strong> Why does this divide-and-conquer algorithm have the same asymptotic complexity as the standard triple-loop algorithm, and why is it often slower in practice?</li>

    <li><strong>Usefulness Without Speedup:</strong> If the asymptotic time is not better than the standard triple-loop method, why might this divide-and-conquer formulation still be valuable in practice? Give at least two concrete advantages.</li>

    <li><strong>Block Correctness:</strong> Briefly justify why the block formulas
      \[
        C_{11}=A_{11}B_{11}+A_{12}B_{21}\] \[
        C_{12}=A_{11}B_{12}+A_{12}B_{22}\] \[
        C_{21}=A_{21}B_{11}+A_{22}B_{21}\] \[
        C_{22}=A_{21}B_{12}+A_{22}B_{22}\]
      are correct. Which algebraic properties of matrix multiplication are used?</li>

    <li><strong>Operation Counts:</strong> Let \(M(n)\) be the number of scalar multiplications and \(A(n)\) the number of scalar additions for this algorithm. Write the recurrences for \(M(n)\) and \(A(n)\) with base cases, and solve them in closed form. Why are these counts important when comparing to Strassen's algorithm?</li>

    <li><strong>Threshold Choice:</strong> Why do implementations switch to the standard triple-loop algorithm below a size threshold? Name two machine- or implementation-specific factors that influence the best threshold value.</li>

    <li><strong>Offsets vs. Copying:</strong> Our implementation uses offsets to operate on subregions in-place rather than allocating submatrices. What asymptotic space benefit does this provide?</li>

    <li><strong>Non-Power-of-2 Sizes:</strong> If \(n\) is not a power of 2 and we pad with zeros to the next power of 2, how does padding affect (a) the correctness of the result and (b) the running time bound in big-O notation?</li>

    <li><strong>Strassen Comparison:</strong> How does Strassen's algorithm improve on the basic divide-and-conquer approach? State its recurrence, its asymptotic time, and one tradeoff that might make the basic approach preferable in some situations.</li>

    <li><strong>Practical Improvements:</strong> What are some practical ways—other than reducing asymptotic complexity—to make matrix multiplication faster on real hardware?</li>
  </ol>

  <button id="toggleAnswers" class="show-answer" aria-expanded="false">Show Answers</button>
  <div id="answers" class="answer" hidden>
    <ol>
      <li><strong>Answer:</strong> The recurrence is \(T(n) = 8T(n/2) + O(n^2)\). Here, \(a = 8\), \(b = 2\), and \(d = 2\) (since \(O(n^2)\)). Since \(a = b^d\) (\(8 = 2^3\)), by the Master Theorem case 1, \(T(n) = \Theta(n^3)\).</li>
      <li><strong>Answer:</strong> Both algorithms perform \(O(n^3)\) work, but the divide-and-conquer version has extra overhead from recursion, function calls, and handling submatrices, making it slower in practice for most sizes.</li>

      <li><strong>Answer:</strong> Advantages include (i) improved cache locality by working on contiguous subblocks, (ii) a clear structure that supports parallelization, and (iii) serving as a foundation for faster algorithms like Strassen's and cache-oblivious methods.</li>

      <li><strong>Answer:</strong> The block formulas follow from associativity and distributivity of matrix multiplication: \((AB)_{ij} = \sum_k A_{ik}B_{kj}\) holds whether \(i,j,k\) index single elements or whole submatrices. Splitting the \(k\)-sum at \(n/2\) yields the two block products per quadrant.</li>

      <li><strong>Answer:</strong> Multiplications: \(M(n) = 8M(n/2)\), \(M(1) = 1\), giving \(M(n) = n^3\). Additions: \(A(n) = 8A(n/2) + 4(n/2)^2\), \(A(1) = 0\), which solves to \(A(n) = n^3 - n^2\). These counts matter because Strassen's reduces multiplications (expensive) at the cost of more additions (cheap).</li>

      <li><strong>Answer:</strong> Small recursive calls have high relative overhead and low arithmetic intensity. Switching to triple-loop at a cutoff reduces constants. Optimal thresholds depend on cache sizes, memory bandwidth, compiler optimizations, and runtime overheads.</li>

      <li><strong>Answer:</strong> Offsets avoid allocating \(O(n^2)\)-sized submatrices at each level, keeping extra space at \(O(\log n)\) for the recursion stack (plus the \(O(n^2)\) output).</li>

      <li><strong>Answer:</strong> (a) Padding with zeros doesn't change the true product in the top-left \(n\times n\) region; trimming yields the correct result. (b) Time complexity is \(O({n'}^3)\) where \(n'\) is the next power of 2 ≥ \(n\); this is still \(O(n^3)\).</li>

      <li><strong>Answer:</strong> Strassen's recurrence is \(T(n) = 7T(n/2) + O(n^2)\), giving \(T(n) = O(n^{\log_2 7}) \approx O(n^{2.81})\). Tradeoff: more additions and complex formulas increase constants and can hurt numerical stability; basic divide-and-conquer may be better for moderate sizes or simple implementations.</li>

      <li><strong>Answer:</strong> Examples: cache-oblivious algorithms to improve memory access patterns, blocking/tiling to fit subblocks in cache, and parallelizing independent subproblems on multiple cores or machines.</li>
    </ol>
  </div>
</section>


<section id="activities" section-title="In-Class Activities">
  <h2>In-Class Activities</h2>
  <ol>
    <li><strong>Block-Correctness Trace (4x4):</strong>
      Partition two 4x4 matrices into 2x2 blocks. Compute
      \(C_{11}=A_{11}B_{11}+A_{12}B_{21}\),
      \(C_{12}=A_{11}B_{12}+A_{12}B_{22}\),
      \(C_{21}=A_{21}B_{11}+A_{22}B_{21}\),
      \(C_{22}=A_{21}B_{12}+A_{22}B_{22}\),
      and verify that these four blocks assemble into the same \(C\) you get from the standard triple-loop method. 
      <em>Deliverable:</em> One worked example with a brief note citing the algebraic properties used (associativity, distributivity).
    </li>

    <li><strong>Exact Operation Counts (with Recurrences):</strong>
      Use the following recurrences for the basic divide-and-conquer algorithm on \(n\times n\) matrices (assume \(n=2^k\)):
      <ul>
        <li>Multiplications: \(M(n)=8\,M(n/2)\), \(M(1)=1\).</li>
        <li>Additions: \(A(n)=8\,A(n/2)+4\,(n/2)^2\), \(A(1)=0\).</li>
      </ul>
      (a) Solve both recurrences in closed form.  
      (b) Verify numerically for \(n\in\{2,4,8\}\).  
      (c) Prove your closed forms by induction on \(k\).  
      <em>Deliverable:</em> Closed forms, small table of values, short induction proof.
    </li>

    <li><strong>Cache-Aware Blocking Thought Experiment:</strong>
      Consider a naive triple-loop implementation and a blocked/tiling version that multiplies subblocks of size \(b\times b\).  
      (a) Explain (qualitatively) why blocking can reduce cache misses.  
      (b) For a given cache that holds \(\Theta(b^2)\) elements, argue how to choose \(b\) to keep two \(b\times b\) tiles of \(A,B\) and one of \(C\) in cache.  
      <em>Deliverable:</em> 5–8 sentence write-up with a simple diagram of data reuse.
    </li>

    <li><strong>Parallel Work/Span Sketch:</strong>
      Model the 8 submultiplications-per-level as a task DAG.  
      (a) What is the total work \(W(n)\) and the span (critical path) \(S(n)\) ignoring additions?  
      (b) Estimate the maximum parallelism \(W(n)/S(n)\).  
      (c) How would this change under Strassen (7 submultiplications)?  
      <em>Deliverable:</em> Brief derivation and a one-level DAG sketch.
    </li>
    <li><strong>Parallel Quadrant Computation:</strong>
  As a group, assign each student (or small team) one of the four quadrants of the result matrix \(C\):
  \(C_{11}\), \(C_{12}\), \(C_{21}\), or \(C_{22}\).  
  Each team:
  <ol type="a">
    <li>Identifies the two submatrix products needed for their quadrant.</li>
    <li>Computes those products (using small matrices for hand calculation) and adds the results.</li>
    <li>Writes their completed quadrant on the board.</li>
  </ol>
  When all teams are done, assemble the full \(C\) matrix and compare to the standard triple-loop computation.  
  <em>Goal:</em> Practice the block multiplication formulas and experience how the algorithm can be parallelized at the quadrant level.
</li>

  </ol>
</section>


  <section id="problems" section-title="Homework Problems">
    <h2>Homework Problems</h2>
    <ol>
      <li><strong>Matrix Transpose (Two Approaches):</strong>  
      The transpose of an \(n\times n\) matrix \(A\) is the matrix \(A^T\) obtained by flipping \(A\) across its main diagonal so that \(A^T[i][j] = A[j][i]\).  
      <ol type="a">
        <li><em>Brute Force:</em> Write pseudocode to compute the transpose of an \(n\times n\) matrix using the straightforward triple-loop method (or double-loop if you swap in place). Analyze its running time and space usage.</li>
        <li><em>Divide and Conquer:</em> Design a recursive divide-and-conquer algorithm for computing the transpose, assuming \(n\) is a power of 2. 
          Give pseudocode using offsets (no copying of submatrices) and analyze its recurrence relation for running time. </li>
          <li>Compare the asymptotic complexity of the two algorithms.</li>
      </ol>
   <li><strong>Recursive Matrix Addition:</strong>  
      Write a divide-and-conquer algorithm to add two \(n\times n\) matrices, assuming \(n\) is a power of 2.  
      <ol type="a">
        <li>Give a clear description and/or pseudocode for your algorithm.</li>
        <li>State the recurrence for the running time.</li>
        <li>Solve it and verify it matches the expected \(O(n^2)\).</li>
        <li>Discuss whether recursion is worthwhile for this problem and why.</li>
      </ol>
    </li>

    <li><strong>Operation Counts for Multiplication:</strong>  
      Using the recurrences  
      \(M(n) = 8M(n/2), \; M(1)=1\) and  
      \(A(n) = 8A(n/2) + 4(n/2)^2, \; A(1) = 0\),  
      <ol type="a">
        <li>Solve for \(M(n)\) and \(A(n)\) in closed form.</li>
        <li>Verify the formulas for \(n=2,4,8\) by explicitly counting operations in small examples.</li>
        <li>Prove your formulas by induction.</li>
      </ol>
    </li>

    <li><strong>Divide-and-Conquer Image Quadrant Processing:</strong>  
      An \(n\times n\) grayscale image is stored in a 2D array.  
      Write a divide-and-conquer algorithm that applies a given pixel operation (e.g., brighten by 10%) to the entire image by recursively processing its four quadrants in parallel.  
      <ol type="a">
        <li>Provide pseudocode.</li>
        <li>Give the recurrence for the running time.</li>
        <li>Compare its asymptotic complexity to simply processing the image in a single (or double-nested) loop.</li>
      </ol>
    </li>

      <li><strong>Handling Non-Square Matrices:</strong>  
      The divide-and-conquer algorithm described in this section assumes square \(n\times n\) matrices with \(n\) a power of 2.  
      Modify the algorithm to multiply two matrices \(A\) of size \(p\times q\) and \(B\) of size \(q\times r\), where \(p, q, r\) are not necessarily equal and may not be powers of 2.  
      <ol type="a">
        <li>Describe how to pad each dimension of \(A\) and \(B\) so the algorithm can still be applied. State exactly how many rows and columns of zeros you need to add to each.</li>
        <li>Explain how to remove the padded rows/columns from the final result to recover the true product.</li>
        <li>Give pseudocode for your modified version.</li>
        <li>Analyze how the padding changes the running time in big-O notation, and discuss when the overhead becomes significant.</li>
      </ol>
    </li>

    <li><strong>Parallel Quadrant Version:</strong>  
      One advantage of the divide-and-conquer approach is that the four quadrants of the result matrix \(C\) can be computed in parallel.  
      <ol type="a">
        <li>Draw a dependency diagram showing which products must be computed for each of the four quadrants \(C_{11}, C_{12}, C_{21}, C_{22}\).</li>
        <li>Write pseudocode that uses four parallel tasks (or threads) to compute the quadrants simultaneously, assuming your language has a simple parallel construct like <code>parallel</code> or <code>spawn</code>/<code>sync</code>.</li>
        <li>Using the parallel work/span model, let \(W(n)\) be the total work (same as the sequential running time) and \(S(n)\) be the span (critical path length). Derive \(W(n)\) and \(S(n)\) for this algorithm, ignoring additions.</li>
        <li>Estimate the maximum theoretical parallelism \(P_{\text{max}} = W(n)/S(n)\) for this algorithm. Explain what this number means for speedup on a real multicore machine.</li>
        <li>Briefly discuss any practical challenges in achieving the theoretical speedup (e.g., overhead, memory bandwidth limits).</li>
      </ol>
    </li>
    </ol>
  </section>

</body>
</html>
